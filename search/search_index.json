{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to BWSI RACECAR!","title":"Home"},{"location":"#welcome-to-bwsi-racecar","text":"","title":"Welcome to BWSI RACECAR!"},{"location":"cheatsheets/opencv/","text":"OpenCV in Python Cheatsheet What to import import cv2 - Imports OpenCV module import numpy as np - Imports Numpy module from cv_bridge import CvBridge, CvBridgeException - Imports cv_bridge to allow changing between ROS messages and images that can be altered with OpenCV. ROS and OpenCV CvBridge() - Instantiates a new bridge object. bridge.imgmsg_to_cv2(image_msg, \"bgr8\") - Converts the image message into an OpenCV image. bridge.cv2_to_imgmsg(image_cv) - Converts the OpenCV image into an image message. cv2.imread('example.jpg') - Reads an image that can be edited using OpenCV. The parameter is the image file. cv2.imshow('image', img) - Displays an image in a window. First parameter is the window name, second is the image. cv2.imwrite('image.png',img) - Saves an image. First parameter is the file name, second is the image to be saved. cv2.waitKey(0) - Waits indefinitely for a key stroke. Needed to process many other GUI events, so is necessary to display the image. cv2.destroyAllWindows() - Destroys all windows created by OpenCV. Other OpenCV functions cv2.line(img ,(0,0), (100,100), (255,0,0), 5) - Draws a line on img, that begins at (0,0) and ends at (100,100), with color (255,0,0) (blue, since it's BGR), and thickness of 5 px. cv2.rectangle(img, (0,100), (150,0), (0,255,0),2) - Draws a rectangle on img with the top left corner at (0,100) and bottom right corner at (150,0), with color (0,255,0) (green) and thickness of 2px. cv2.resize(img, (int (img.shape[1]/4)), (int (img.shape[0]/4))) - Resizes img to be a 1/4th its width (img.shape[1]) and 1/4th its height (img.shape[0]). cv2.putText(img, 'hello', (150,150), textFont, 5, (255,255,255), 2, cv2.LINE_AA) - Puts the text \"hello\" on img at (150,150) (bottom-left corner), in font textFont with scale 5, in color (255,255,255) (white), with thickness of 2 px and lineType of cv2.LINE_AA. cv2.cvtColor(img, cv2.COLOR_BGR2HSV) - Converts img from the BGR color space to HSV. cv2.inRange(hsvImage, lower, upper) - Thresholds the HSV image to get only the colors that fall within the lower and upper bounds, where lower and upper are numpy arrays that specify HSV values. cv2.bitwise_and(img, img, mask=imgMask) - Masks the original image with imgMask being the result of cv2.inRange(). cv2.findContours(img,cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) - Finds contours. First parameter is the source image, second is contour retrieval mode, and third is contour approximation method. Returns the modfied image, the contours, and hierarchy. cv2.drawContours(img, [cont], 0, (0,255,0),3) - Draw contours on img in the color (0,255,0) (green) with thickness of 3px. [cont] is a list of the contours, and 0 is the index of contours (for drawing individual ones). To draw all contours, pass -1 as the third argument. Sources OpenCV Documentation","title":"OpenCV"},{"location":"cheatsheets/opencv/#opencv-in-python-cheatsheet","text":"","title":"OpenCV in Python Cheatsheet"},{"location":"cheatsheets/opencv/#what-to-import","text":"import cv2 - Imports OpenCV module import numpy as np - Imports Numpy module from cv_bridge import CvBridge, CvBridgeException - Imports cv_bridge to allow changing between ROS messages and images that can be altered with OpenCV.","title":"What to import"},{"location":"cheatsheets/opencv/#ros-and-opencv","text":"CvBridge() - Instantiates a new bridge object. bridge.imgmsg_to_cv2(image_msg, \"bgr8\") - Converts the image message into an OpenCV image. bridge.cv2_to_imgmsg(image_cv) - Converts the OpenCV image into an image message. cv2.imread('example.jpg') - Reads an image that can be edited using OpenCV. The parameter is the image file. cv2.imshow('image', img) - Displays an image in a window. First parameter is the window name, second is the image. cv2.imwrite('image.png',img) - Saves an image. First parameter is the file name, second is the image to be saved. cv2.waitKey(0) - Waits indefinitely for a key stroke. Needed to process many other GUI events, so is necessary to display the image. cv2.destroyAllWindows() - Destroys all windows created by OpenCV.","title":"ROS and OpenCV"},{"location":"cheatsheets/opencv/#other-opencv-functions","text":"cv2.line(img ,(0,0), (100,100), (255,0,0), 5) - Draws a line on img, that begins at (0,0) and ends at (100,100), with color (255,0,0) (blue, since it's BGR), and thickness of 5 px. cv2.rectangle(img, (0,100), (150,0), (0,255,0),2) - Draws a rectangle on img with the top left corner at (0,100) and bottom right corner at (150,0), with color (0,255,0) (green) and thickness of 2px. cv2.resize(img, (int (img.shape[1]/4)), (int (img.shape[0]/4))) - Resizes img to be a 1/4th its width (img.shape[1]) and 1/4th its height (img.shape[0]). cv2.putText(img, 'hello', (150,150), textFont, 5, (255,255,255), 2, cv2.LINE_AA) - Puts the text \"hello\" on img at (150,150) (bottom-left corner), in font textFont with scale 5, in color (255,255,255) (white), with thickness of 2 px and lineType of cv2.LINE_AA. cv2.cvtColor(img, cv2.COLOR_BGR2HSV) - Converts img from the BGR color space to HSV. cv2.inRange(hsvImage, lower, upper) - Thresholds the HSV image to get only the colors that fall within the lower and upper bounds, where lower and upper are numpy arrays that specify HSV values. cv2.bitwise_and(img, img, mask=imgMask) - Masks the original image with imgMask being the result of cv2.inRange(). cv2.findContours(img,cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) - Finds contours. First parameter is the source image, second is contour retrieval mode, and third is contour approximation method. Returns the modfied image, the contours, and hierarchy. cv2.drawContours(img, [cont], 0, (0,255,0),3) - Draw contours on img in the color (0,255,0) (green) with thickness of 3px. [cont] is a list of the contours, and 0 is the index of contours (for drawing individual ones). To draw all contours, pass -1 as the third argument.","title":"Other OpenCV functions"},{"location":"cheatsheets/opencv/#sources","text":"OpenCV Documentation","title":"Sources"},{"location":"cheatsheets/python/","text":"ROS in Python cheatsheet What to import import rospy - To get all of the essential ROS classes from [message folder].msg import [name of message type] - To import a message type (use * to get all of them from the folder) from cv_bridge import CvBridge, CvBridgeException - To get the bridge to translate from image msg to OpenCV image rospy classes and functions to use rospy.init_node(node_name) - Initializes the node onto the ROS Graph rospy.Subscriber - A subscriber will allow receiving messages from a topic onto a callback function Init: sub = rospy.Subscriber(topic_name, message_type, callback_function_name, queue_size = _) - Initializes with a topic and message type onto a callback function which is called whenever it is published onto Callback Function: def callback_function_name([self if class], message_type): - The callback function is given the data from the message rospy.Publisher - Publishers allow us to publish messages onto a topic Init: pub = rospy.Publisher(topic_name, message_type, queue_size = _) - Initialize it as an object with a topic and a message type (can already be existing) Publish: pub.publish([variable of message_type type]) - Publishes the data rospy.is_shutdown() - While the current roscore is still running rospy.spin() - Gives the functionality to ROS until roscore is shut down Python Conventions if __name__ == \"__main__\": - Acts as a main function, only runs if python [script_name] is used on it Naming variable_name (underscores) ClassName (CamelCase) 4 spaces per tab Put all functionality in a class (?)","title":"Python"},{"location":"cheatsheets/python/#ros-in-python-cheatsheet","text":"","title":"ROS in Python cheatsheet"},{"location":"cheatsheets/python/#what-to-import","text":"import rospy - To get all of the essential ROS classes from [message folder].msg import [name of message type] - To import a message type (use * to get all of them from the folder) from cv_bridge import CvBridge, CvBridgeException - To get the bridge to translate from image msg to OpenCV image","title":"What to import"},{"location":"cheatsheets/python/#rospy-classes-and-functions-to-use","text":"rospy.init_node(node_name) - Initializes the node onto the ROS Graph rospy.Subscriber - A subscriber will allow receiving messages from a topic onto a callback function Init: sub = rospy.Subscriber(topic_name, message_type, callback_function_name, queue_size = _) - Initializes with a topic and message type onto a callback function which is called whenever it is published onto Callback Function: def callback_function_name([self if class], message_type): - The callback function is given the data from the message rospy.Publisher - Publishers allow us to publish messages onto a topic Init: pub = rospy.Publisher(topic_name, message_type, queue_size = _) - Initialize it as an object with a topic and a message type (can already be existing) Publish: pub.publish([variable of message_type type]) - Publishes the data rospy.is_shutdown() - While the current roscore is still running rospy.spin() - Gives the functionality to ROS until roscore is shut down","title":"rospy classes and functions to use"},{"location":"cheatsheets/python/#python-conventions","text":"if __name__ == \"__main__\": - Acts as a main function, only runs if python [script_name] is used on it Naming variable_name (underscores) ClassName (CamelCase) 4 spaces per tab Put all functionality in a class (?)","title":"Python Conventions"},{"location":"cheatsheets/ros-topics-msgs/","text":"ROS Helpful Topic and Message Type Cheatsheet Topics Ex. topic_name ( message_type_name ) - Description of topic Sensor Topics /scan ( sensor_msgs/LaserScan ) - Topic that the Hokuyo LIDAR sends /scan_processed - Topic that the processed Velodyne LIDAR sends /imu/data ( sensor_msgs/Imu ) - Topic that the IMU sends /tf ( tf/tfMessage ) - Topic that shows the various transformed coordinate systems /odom ( geometry_msgs/Point ) - Topic that shows the Odometry data Camera Topics /zed/rgb/image_rect_color ( sensor_msgs/Image ) - Topic that gives the rectified image of the camera (in full color) /zed/[right or left]/image_raw_color ( sensor_msgs/Image ) - Topic that gives the raw image of either of the right or left cameras (in full color) /zed/depth/depth_registered ( sensor_msgs/Image ) - Topic that gives the depth data from the camera (black and white) External Usage Topics /vesc/high_level/ackermann_cmd_mux/input/nav_0 ( ackermann_msgs/AckermannDriveStamped ) - What to write to in order to run the car (without bypassing the safety controller) Safety Controller Topics /ackermann_cmd_input ( ackermann_msgs/AckermannDriveStamped ) - The input about to be published to run the RACECAR /ackermann_cmd ( ackermann_msgs/AckermannDriveStamped ) - The output to actually drive the car (without anymore checking involved) AR Topics /visualization_marker ( visualization_msgs/Marker ) - /ar_pose_marker ( ar_track_alvar_msgs/AlvarMarkers ) - Gives an array of the AlvarMarkers detected Messages Ex. from message_folder import message_type - What used for * important_sub_topic - what represents Sensor Messages from sensor_msgs.msg import LaserScan - The information coming from the LIDAR LaserScan.ranges - An array of 1081 distances coming from the laser scan from sensor_msgs.msg import Image - The image published by the camera Ackermann Messages from ackermann_msgs.msg import AckermannDriveStamped - The drive command that the Ackermann command mux takes in","title":"ROS Topics and Messages"},{"location":"cheatsheets/ros-topics-msgs/#ros-helpful-topic-and-message-type-cheatsheet","text":"","title":"ROS Helpful Topic and Message Type Cheatsheet"},{"location":"cheatsheets/ros-topics-msgs/#topics","text":"Ex. topic_name ( message_type_name ) - Description of topic","title":"Topics"},{"location":"cheatsheets/ros-topics-msgs/#sensor-topics","text":"/scan ( sensor_msgs/LaserScan ) - Topic that the Hokuyo LIDAR sends /scan_processed - Topic that the processed Velodyne LIDAR sends /imu/data ( sensor_msgs/Imu ) - Topic that the IMU sends /tf ( tf/tfMessage ) - Topic that shows the various transformed coordinate systems /odom ( geometry_msgs/Point ) - Topic that shows the Odometry data","title":"Sensor Topics"},{"location":"cheatsheets/ros-topics-msgs/#camera-topics","text":"/zed/rgb/image_rect_color ( sensor_msgs/Image ) - Topic that gives the rectified image of the camera (in full color) /zed/[right or left]/image_raw_color ( sensor_msgs/Image ) - Topic that gives the raw image of either of the right or left cameras (in full color) /zed/depth/depth_registered ( sensor_msgs/Image ) - Topic that gives the depth data from the camera (black and white)","title":"Camera Topics"},{"location":"cheatsheets/ros-topics-msgs/#external-usage-topics","text":"/vesc/high_level/ackermann_cmd_mux/input/nav_0 ( ackermann_msgs/AckermannDriveStamped ) - What to write to in order to run the car (without bypassing the safety controller)","title":"External Usage Topics"},{"location":"cheatsheets/ros-topics-msgs/#safety-controller-topics","text":"/ackermann_cmd_input ( ackermann_msgs/AckermannDriveStamped ) - The input about to be published to run the RACECAR /ackermann_cmd ( ackermann_msgs/AckermannDriveStamped ) - The output to actually drive the car (without anymore checking involved)","title":"Safety Controller Topics"},{"location":"cheatsheets/ros-topics-msgs/#ar-topics","text":"/visualization_marker ( visualization_msgs/Marker ) - /ar_pose_marker ( ar_track_alvar_msgs/AlvarMarkers ) - Gives an array of the AlvarMarkers detected","title":"AR Topics"},{"location":"cheatsheets/ros-topics-msgs/#messages","text":"Ex. from message_folder import message_type - What used for * important_sub_topic - what represents","title":"Messages"},{"location":"cheatsheets/ros-topics-msgs/#sensor-messages","text":"from sensor_msgs.msg import LaserScan - The information coming from the LIDAR LaserScan.ranges - An array of 1081 distances coming from the laser scan from sensor_msgs.msg import Image - The image published by the camera","title":"Sensor Messages"},{"location":"cheatsheets/ros-topics-msgs/#ackermann-messages","text":"from ackermann_msgs.msg import AckermannDriveStamped - The drive command that the Ackermann command mux takes in","title":"Ackermann Messages"},{"location":"cheatsheets/ros/","text":"ROS Command Line cheatsheet Launching ROS roscore - Allows all ROS nodes to communicate with each other, so necessary for all ROS usage rosrun [package] [executable] - Launches a ROS node, executable can be a .py file or a C/C++ executable roslaunch [package] [launch_script] - Uses a .launch file to run multiple other ROS nodes and roscore Debugging Information rosnode [command] - Displays debugging information about ROS nodes rosnode list - Lists the active nodes rosnode ping [node_name] - Tests connectivity to node rosnode info [node_name] - Prints information about specific node rosnode machine [machine_name] - Lists nodes running on a machine rosnode kill [node_name] - Kills a running node -a kills all nodes rostopic [command] - Displays debugging information about ROS topics rostopic list - Lists the active topics rostopic bw [topic_name] - Prints the bandwidth used by topic rostopic hz [topic_name] - Prints the publishing rate of the topic rostopic echo [topic_name] - Print topic messages to the screen rostopic type [topic_name] - Print topic's message type rostopic pub [topic_name] [message type] (message) - Publish data to the topic rostopic find [message_type] - Find topics by message type rosmsg [command] - Displays information about ROS message data structures rosmsg show [message_name] - Print the fields in the message rosmsg users [message_name] - Search for code using the message rosmsg package [package_name] - List all the messages in a package rosmsg packages [message_name] - List all the packages with specific message rosmsg md5 [message_name] - Print the message md5 sum Recording rosbag [command] - Used for storing ROS graph data rosbag record [topics_names] - Records the ROS data into a .bag file (-a records all topics) rosbag play [.bag file] - Plays back the ROS data that was recorded Visualization Tools rqt_graph - Displays interactive graph of ROS nodes/topics rqt_image_view - Displays any topic messages whose type is sensor_msgs/Image rqt_bag - Graphical tool for watching .bag files rqt_deps - Generates PDF of ROS dependencies rqt_plot - Plots numerical data on ROS topic over time rqt_logger rviz - 3D visualization of robot with sensor data plotted around it gazebo - A simulation system where the ROS robot can operate just like it normally would","title":"ROS Terminal Commands"},{"location":"cheatsheets/ros/#ros-command-line-cheatsheet","text":"","title":"ROS Command Line cheatsheet"},{"location":"cheatsheets/ros/#launching-ros","text":"roscore - Allows all ROS nodes to communicate with each other, so necessary for all ROS usage rosrun [package] [executable] - Launches a ROS node, executable can be a .py file or a C/C++ executable roslaunch [package] [launch_script] - Uses a .launch file to run multiple other ROS nodes and roscore","title":"Launching ROS"},{"location":"cheatsheets/ros/#debugging-information","text":"rosnode [command] - Displays debugging information about ROS nodes rosnode list - Lists the active nodes rosnode ping [node_name] - Tests connectivity to node rosnode info [node_name] - Prints information about specific node rosnode machine [machine_name] - Lists nodes running on a machine rosnode kill [node_name] - Kills a running node -a kills all nodes rostopic [command] - Displays debugging information about ROS topics rostopic list - Lists the active topics rostopic bw [topic_name] - Prints the bandwidth used by topic rostopic hz [topic_name] - Prints the publishing rate of the topic rostopic echo [topic_name] - Print topic messages to the screen rostopic type [topic_name] - Print topic's message type rostopic pub [topic_name] [message type] (message) - Publish data to the topic rostopic find [message_type] - Find topics by message type rosmsg [command] - Displays information about ROS message data structures rosmsg show [message_name] - Print the fields in the message rosmsg users [message_name] - Search for code using the message rosmsg package [package_name] - List all the messages in a package rosmsg packages [message_name] - List all the packages with specific message rosmsg md5 [message_name] - Print the message md5 sum","title":"Debugging Information"},{"location":"cheatsheets/ros/#recording","text":"rosbag [command] - Used for storing ROS graph data rosbag record [topics_names] - Records the ROS data into a .bag file (-a records all topics) rosbag play [.bag file] - Plays back the ROS data that was recorded","title":"Recording"},{"location":"cheatsheets/ros/#visualization-tools","text":"rqt_graph - Displays interactive graph of ROS nodes/topics rqt_image_view - Displays any topic messages whose type is sensor_msgs/Image rqt_bag - Graphical tool for watching .bag files rqt_deps - Generates PDF of ROS dependencies rqt_plot - Plots numerical data on ROS topic over time rqt_logger rviz - 3D visualization of robot with sensor data plotted around it gazebo - A simulation system where the ROS robot can operate just like it normally would","title":"Visualization Tools"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/","text":"Lab: RACECAR Hardware Intro Welcome to the RACECAR ( R apid A utonomous C omplex- E nvironment C ompeting A ckermann-steering R obot)! Turning on the Car Use Blocko Friendo Set the car on a block on your table, such that the wheels aren't touching the table . Unless you're actually testing your code on the ground, leave the car on this block. Plug in the Electronics Battery Unplug the battery charger. If you have an XT Power battery, there will be two identical wires; you only need to plug one into the battery. On these batteries, quickly press the battery's power button. If you hold it, it will change the voltage setting. If you have an Energizer battery, there will be a wire marked blue and a wire marked green, which you must plug into the corresponding ports). Troubleshooting Tip: if something isn't receivng power, check to make sure that the blue and green ports inside the Energizer aren't broken apart. Each should have a center pin. Boot it Up Turn on the car using the power button on the TX2. The lights should turn green. Plug in the Traxxas Battery This battery can supply ample amperage to the VESC (Vedder Electronic Speed Controller) which controls the drive and steering motor. Troubleshooting Tip: you'll know the VESC is receiving power if the steering servo motor tries to keep the front wheels steered forwards. Also, please don't apply too much force when trying to turn the front wheels to test this; it doesn't take much force to tell. SSH'ing The ssh command allows you to use a remote computer's (i.e. a racecar's) terminal on your host machine. To ssh, make sure you are connected to the same wifi network as your car. Then run in your terminal: ssh racecar@192.168.1.<YOUR_CAR_NUMBER> . The RACECAR's password is racecar@mit. Make sure everyone on your team is able to do this before moving on! SCP'ing The scp command allows you to copy files back and forth from a remote machine. From local to remote (i.e. upload to car): scp <LOCAL_FILE_PATH> <REMOTE_USERNAME>@<IP>:<REMOTE_FILE_PATH> From remote to local (i.e. download from car): scp <REMOTE_USERNAME>@<IP>:<REMOTE_FILE_PATH> <LOCAL_FILE_PATH> For example, if you wanted to move a file on your Mac desktop called \"drive.py\" to the \"racecar_ws\" folder of car 101, you would run: scp ~/Desktop/drive.py racecar@192.168.1.101:/home/racecar/racecar_ws/ Troubleshooting Tip: You should run scp outside of the racecar (i.e. not in an ssh'd terminal). Otherwise, you'd be flipped around and need your computer's hostname and IP. Controlling the car Once you've successfully ssh'd in, type teleop . This will allow us to actually drive the car. If there is an error about it being unable to connect to the joystick, end the program (ctrl-C) and try again. If there is still an error, as a TA for help. Now you can drive the car using the controller! The cars have a Dead Man's Switch for safety; this means that to use the controller to drive, you have to hold down LB at the same time. If the Mode light is turned on, press Mode. The controller must also be in the X mode (this is changed by a switch in between the bumpers). Try driving around! The left joystick controls speed, and the right joystick turns. Keep in mind that these are expensive (i.e. several kilodollars!) piece of equipment; always be aware of your/the car's surroudings when driving, and communicate with other teams that are also driving their cars to avoid damaging them. Common Errors If your car doesn't drive with the controller... Check teleop for any errors. There should be a \"force feedback\" error for the joystick, but everything else should run fine. If there is a VESC error, quit teleop , wait a minute or two and try again. This often happens if you try to run teleop immediately after turning on the car or pluggin in the Traxxas battery, since it takes a minute for the VESC to boot up. If you get individual errors for the Hokuyo or IMU, check that these are properly plugged in. The IMU should be plugged into the USB hub, and its micro USB port should not be ripped off. It should light a blue LED when it's powered on. For the Hokuyo, check that its Ethernet cable is plugged into the USB and that it's power cable is plugged into the power harness. You'll know the Hokuyo is receiving power if you can hear it whirring. If you get multiple errors for the VESC and IMU, there might be a problem with your USB hub. Try power-cycling the car (turning it off and on). If that doesn't fix it, check that the USB hub is connected to the TX2 board and is getting power from the electronics battery through the power harness. Alternatively, you may also get a similar whopping load of errors if more than one person tries running teleop at the same time. Power cycle the car and try it again with only one person ssh-ed in (or just use the monitor and keyboard). If your controller was on D and then you switched it to X, try power-cycling the car. When you think you have a good feel of the car, move on to the next section. Intro to autonomous driving First, let's just program the racecar to drive forwards on its own. Open controller.py . Change the car's velocity in the function drive_callback . You can change the speed with the variable self.cmd.drive.speed . Start off with small values like 0.5 so you don't accidently crash! When you want to run your code on the car, press the right bumper on the controller after teleop and your code are running. Next, try having the car drive in a circle. You can change the wheel angle with the variable self.cmd.drive.steering_angle (the car uses radians). Explore the range of values that might work best for your car (speed, turning angle, etc).","title":"Using the Hardware"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#turning-on-the-car","text":"Use Blocko Friendo Set the car on a block on your table, such that the wheels aren't touching the table . Unless you're actually testing your code on the ground, leave the car on this block. Plug in the Electronics Battery Unplug the battery charger. If you have an XT Power battery, there will be two identical wires; you only need to plug one into the battery. On these batteries, quickly press the battery's power button. If you hold it, it will change the voltage setting. If you have an Energizer battery, there will be a wire marked blue and a wire marked green, which you must plug into the corresponding ports). Troubleshooting Tip: if something isn't receivng power, check to make sure that the blue and green ports inside the Energizer aren't broken apart. Each should have a center pin. Boot it Up Turn on the car using the power button on the TX2. The lights should turn green. Plug in the Traxxas Battery This battery can supply ample amperage to the VESC (Vedder Electronic Speed Controller) which controls the drive and steering motor. Troubleshooting Tip: you'll know the VESC is receiving power if the steering servo motor tries to keep the front wheels steered forwards. Also, please don't apply too much force when trying to turn the front wheels to test this; it doesn't take much force to tell.","title":"Turning on the Car"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#sshing","text":"The ssh command allows you to use a remote computer's (i.e. a racecar's) terminal on your host machine. To ssh, make sure you are connected to the same wifi network as your car. Then run in your terminal: ssh racecar@192.168.1.<YOUR_CAR_NUMBER> . The RACECAR's password is racecar@mit. Make sure everyone on your team is able to do this before moving on!","title":"SSH'ing"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#scping","text":"The scp command allows you to copy files back and forth from a remote machine. From local to remote (i.e. upload to car): scp <LOCAL_FILE_PATH> <REMOTE_USERNAME>@<IP>:<REMOTE_FILE_PATH> From remote to local (i.e. download from car): scp <REMOTE_USERNAME>@<IP>:<REMOTE_FILE_PATH> <LOCAL_FILE_PATH> For example, if you wanted to move a file on your Mac desktop called \"drive.py\" to the \"racecar_ws\" folder of car 101, you would run: scp ~/Desktop/drive.py racecar@192.168.1.101:/home/racecar/racecar_ws/ Troubleshooting Tip: You should run scp outside of the racecar (i.e. not in an ssh'd terminal). Otherwise, you'd be flipped around and need your computer's hostname and IP.","title":"SCP'ing"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#controlling-the-car","text":"Once you've successfully ssh'd in, type teleop . This will allow us to actually drive the car. If there is an error about it being unable to connect to the joystick, end the program (ctrl-C) and try again. If there is still an error, as a TA for help. Now you can drive the car using the controller! The cars have a Dead Man's Switch for safety; this means that to use the controller to drive, you have to hold down LB at the same time. If the Mode light is turned on, press Mode. The controller must also be in the X mode (this is changed by a switch in between the bumpers). Try driving around! The left joystick controls speed, and the right joystick turns. Keep in mind that these are expensive (i.e. several kilodollars!) piece of equipment; always be aware of your/the car's surroudings when driving, and communicate with other teams that are also driving their cars to avoid damaging them.","title":"Controlling the car"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#common-errors","text":"If your car doesn't drive with the controller... Check teleop for any errors. There should be a \"force feedback\" error for the joystick, but everything else should run fine. If there is a VESC error, quit teleop , wait a minute or two and try again. This often happens if you try to run teleop immediately after turning on the car or pluggin in the Traxxas battery, since it takes a minute for the VESC to boot up. If you get individual errors for the Hokuyo or IMU, check that these are properly plugged in. The IMU should be plugged into the USB hub, and its micro USB port should not be ripped off. It should light a blue LED when it's powered on. For the Hokuyo, check that its Ethernet cable is plugged into the USB and that it's power cable is plugged into the power harness. You'll know the Hokuyo is receiving power if you can hear it whirring. If you get multiple errors for the VESC and IMU, there might be a problem with your USB hub. Try power-cycling the car (turning it off and on). If that doesn't fix it, check that the USB hub is connected to the TX2 board and is getting power from the electronics battery through the power harness. Alternatively, you may also get a similar whopping load of errors if more than one person tries running teleop at the same time. Power cycle the car and try it again with only one person ssh-ed in (or just use the monitor and keyboard). If your controller was on D and then you switched it to X, try power-cycling the car. When you think you have a good feel of the car, move on to the next section.","title":"Common Errors"},{"location":"curriculum/mod0/01_hardware_intro/hardware_intro/#intro-to-autonomous-driving","text":"First, let's just program the racecar to drive forwards on its own. Open controller.py . Change the car's velocity in the function drive_callback . You can change the speed with the variable self.cmd.drive.speed . Start off with small values like 0.5 so you don't accidently crash! When you want to run your code on the car, press the right bumper on the controller after teleop and your code are running. Next, try having the car drive in a circle. You can change the wheel angle with the variable self.cmd.drive.steering_angle (the car uses radians). Explore the range of values that might work best for your car (speed, turning angle, etc).","title":"Intro to autonomous driving"},{"location":"curriculum/mod0/02_ros_intro/ros_intro/","text":"Lab: Intro to Terminal & ROS Personal note: We realize some of you may find this a bit slow/pedantic, but please know that we aren\u2019t doing this just to annoy you. Knowledge being what it is, it is often necessary to learn a language before trying to write with it. And if you do enjoy the language itself, that\u2019s cool beans too! Manipulating Files Using the Terminal Make sure you have an installation of ROS. If you installed ROS on your (presumably Ubuntu) computer or if you have a monitor plugged in to your car, you are good to go. Otherwise, run Docker (because the fishberg/racecar image has an installation of ROS) and mount to a folder on your host machine. (Good luck! Hehe.) Use terminal commands to navigate to a folder that holds programs. The terminal command for navigating to a folder is cd <path_to_folder>/ . If using a racecar, navigate to the \"~/racecar_ws/src/scripts/\" folder. If using your own computer, navigate to wherever makes sense. If using Docker on your computer, navigate to your mount folder (in either your host machine terminal or the Docker machine terminal). Some Common Folders, For Reference \"/\" is the folder that holds all other folders \"~/\" is the home folder \"./\" is the current folder \"../\" is the folder that holds the current folder \"~/racecar_ws/src/scripts/\" is a good folder for store programs on the racecars Create a new folder for this lab's programs, and name it \"example_ROS_files\". The terminal command is mkdir <new_folder_name> . Download the ROS lab starter files from here on Google Drive and move them to the folder you just created. The terminal command for moving files is mv <folder_path_to_copy_from>/<file_name> <folder_path_to_copy_into>/<new_file_name> . If you're directly on the car, make sure that your car's router is plugged into an active ethernet port, or just connect the car to Wifi network. If you're using Docker, you'll want to do this on your host machine. If you've mounted correctly, the files should also appear in the Docker machine's mount folder. Creating a ROS Network from Scratch In a terminal that has ROS, run roscore . In short, roscore is a sort of master node looks at all the other nodes\u2019 publishers and subscribers and connects them appropriately. See the \"Connecting Nodes\" section of the ROS Reference Page . Take a look at myBasicNode1.py and try to predict what it does. (i.e. open it with some text editor) If you still aren\u2019t sure what\u2019s going on after about 5 min, just move on to the next step. In another window/tab of terminal, run myBasicNode1.py. The terminal command is <path_to_file>/<python_file_name> . This works because we have #!/usr/bin/env python at the top of our python files. Otherwise we would need to use python <path_to_file>/<python_file_name> to run our file as a python program. Does your code not run at all (no Python errors or anything)? Has your permission been denied? Well then folks, it\u2019s debugging time! What\u2019s probably going on is you don\u2019t have the permissions set to let your programs be executable. We can check this by trying ls -la while in your program\u2019s folder. If your terminal spits back \"-rw-r--r--\" preceding your filename (\"myBasicNode1.py\"), that is indeed what's happening. Add e x ecutable permission by running chmod +x <file_name> . Hopefully you'll see something different if you run ls -la again. Try rosnode list , then rostopic list , then rostopic echo blah , then rostopic echo blah/data . Then take a look at \"myBasicNode1.py\" again. Just use one more window/tab; you don\u2019t need to run these all at the same time. Where exactly in \"myBasicNode1.py\" do you see some of the names that rosnode list and rostopic list spat out? Take note of the small difference between \"blah\" and \"blah/data\". It will become more important as we begin to deal with messages that have many attributes. Now if there\u2019s a detail about the node that still doesn\u2019t make sense, please ask your squad members or your TA\u2019s what\u2019s the situation here because the rest of the lab builds upon this. See the ROS Command Line Cheatsheet for more fun and informative ROS commands. Now take a look at myNode1.py and fill it in to make it work like myBasicNode1.py. In case you haven\u2019t heard of classes before, we created a class in \"myNode1.py\". Used this way, the class structure simply helps organization. We will not expect you to write classes from scratch, but that said, classes are good to know if you plan on working with object-oriented programming long-term. Now fill out \"myNode2.py\" so that it subscribes to the \"blah\" topic and prints out the messages it receives. Run it at the same time as \"myNode1.py\". See the \"Subscribers\" section of the ROS Reference Page for a review of subscribers and callback functions if needed. Say you called your messages \"msg\". Try printing \"msg\" versus \"msg.data\" to tell the difference between the two. You should see a similar difference as to when you tried echoing \"blah\" vs \"blah/data\". While the nodes are running, try rqt_graph . Is this the graph you anticipate? If not, please ask your squad members or your TA\u2019s what\u2019s going on here because this is decently important to understanding ROS.","title":"Intro to Terminal & ROS"},{"location":"curriculum/mod1/01_proportional_controller/proportional_controller/","text":"Lab: Proportional Control Let's make the car autonomously react to its environment! Probem Statement The racecar's goal will be to maintain a certain distance away from an object in front of it. We will use proportional control to implement this behavior. Idea Behind the Algorithm A simple, non-propotional way to implement the behavior would be (pardon the pythonic pseudocode): if (distance>desired): motor_power(1) else: motor_power(-1) This is a bang-bang controller. It would work, but not efficiently. It tries to run at constant speeds, so even if the speed is reasonable (even a tad slow) when the car is driving normally, the car would really jerk around when switches directions. (Don't believe us? Try it out.) So what could we do to improve this controller? We could add a greater variety of speeds: if (distance>desired+10): motor_power(2) elif (disance>desired+5): motor_power(1) elif (distance>desired+1): motor_power(0.2) elif (distance<desired-10): motor_power(-2) elif (distance<desired-5): motor_power(-1) elif (distance<desired-1): motor_power(-0.2) else: motor_power(0) In this model, when the car is far away, from the desired distance, it can run at a fast speed of 2, but as it gets closer to the desired distance, it slows down to 1, then 0.2, and finally switches direction at 0. This is an improvement. But we could improve it even more by making continuous changes as opposed to these discrete bang-bang steps. Your goal is to make the speed directly proportional to distance away (or the \"error\") from the desired distance. Implementation Choose a distance threshold, probably between 0.5 and 1.5 meters. Have the car aim to stay this distance away from an object in front of it. Of course, this will only allow the car to follow it in a straight line. If you have extra time, you can use this same idea to adjust the angle of the wheels so the car will stay squared to the object as it moves. Work in controller.py . Instead of working in drive_callback , write a new function propControl , then publish in the callback. If you have questions about getting distance measurements from the LIDAR, see the Usage in ROS section of the LIDAR reference page.","title":"Proportional Control"},{"location":"curriculum/mod1/01_proportional_controller/proportional_controller/#probem-statement","text":"The racecar's goal will be to maintain a certain distance away from an object in front of it. We will use proportional control to implement this behavior.","title":"Probem Statement"},{"location":"curriculum/mod1/01_proportional_controller/proportional_controller/#idea-behind-the-algorithm","text":"A simple, non-propotional way to implement the behavior would be (pardon the pythonic pseudocode): if (distance>desired): motor_power(1) else: motor_power(-1) This is a bang-bang controller. It would work, but not efficiently. It tries to run at constant speeds, so even if the speed is reasonable (even a tad slow) when the car is driving normally, the car would really jerk around when switches directions. (Don't believe us? Try it out.) So what could we do to improve this controller? We could add a greater variety of speeds: if (distance>desired+10): motor_power(2) elif (disance>desired+5): motor_power(1) elif (distance>desired+1): motor_power(0.2) elif (distance<desired-10): motor_power(-2) elif (distance<desired-5): motor_power(-1) elif (distance<desired-1): motor_power(-0.2) else: motor_power(0) In this model, when the car is far away, from the desired distance, it can run at a fast speed of 2, but as it gets closer to the desired distance, it slows down to 1, then 0.2, and finally switches direction at 0. This is an improvement. But we could improve it even more by making continuous changes as opposed to these discrete bang-bang steps. Your goal is to make the speed directly proportional to distance away (or the \"error\") from the desired distance.","title":"Idea Behind the Algorithm"},{"location":"curriculum/mod1/01_proportional_controller/proportional_controller/#implementation","text":"Choose a distance threshold, probably between 0.5 and 1.5 meters. Have the car aim to stay this distance away from an object in front of it. Of course, this will only allow the car to follow it in a straight line. If you have extra time, you can use this same idea to adjust the angle of the wheels so the car will stay squared to the object as it moves. Work in controller.py . Instead of working in drive_callback , write a new function propControl , then publish in the callback. If you have questions about getting distance measurements from the LIDAR, see the Usage in ROS section of the LIDAR reference page.","title":"Implementation"},{"location":"curriculum/mod1/02_wall_follower/wall_follower/","text":"Competitive Lab: Wall Following Drag Race Vroom Vroom Problem Statement It's time for your first challenge! You will navigate a racetrack using wall following. We will have two cars running simultaneously between two walls with \"lanes\" made of tape: We will time each team, and if you stray outside of your lane, you will get a time penalty (the lanes are about 3 cars wide, so it won't hopefully won't be too difficult). Idea Behind the Algorithm Wall following is a fairly simple mode of autonomous navigation where your car will drive alongside a wall. There are multiple ways to do this. The simplest way would be to keep the car a certain distance from an obstacle to the side of the car (using a proportional or PID controller). Another strategy is to first find what is most likely to be a wall, then use it to predict the path of the wall. You could try to do a full linear regression, but this probably isn't the most efficient method. You can also do a simple \"bang-bang\" method and pick two random close points that seem to represent a wall, then have the car follow the line that passes through them. Implementation Here are some tips for a successful wall follower: Consider slicing the ranges data into more useful pieces. A majority of the data won\u2019t be useful to you if you only care about a wall to one side. When you can, try to use numpy operations rather than for loops in your code. Multidimensional slicing and broadcasting can make your code cleaner and much more efficient. You can turn any array into a numpy array with np.array , or you can integrate it directly with ROS like in this tutorial . Start off just looking for a wall on one side of the car; once this side works, implement it for the other side as well. You can map the left and right walls to different button (ex. 'A' activates the left wall follower, and 'B' activates the right wall follower). All the joystick buttons correspond to an integer in its ROS message; check the joystick callback function or run rostopic echo vesc/joy for hints on how to map buttons. Convert LIDAR points to cartesian points to calculate their resulting line. Make a separate function to do this. Be warned that if your algorithm only tries to make the car parallel to the wall, the car's average distance from the wall may develop cumulative error. Extra Challenges : Make the car autonomously decide which wall it should follow. You might base this off of factors like the distance or the accuracy of the predicted wall. If you also complete this, you could try to make your wall follower more robust, or try to implement a PID to make it smoother and more accurate. To keep your code organized and efficient, we've set up empty functions for you. I recommend only using drive_callback to set the driving speed and angle and putting everything else in other function. If there is too much code in a callback, it will become too slow and the car won't drive smoothly. Functions to write: convertPoint : convert polar points (points with only magnitude and angle) to cartesian coordinates estimateWall : Predict the wall using the method described above. Currently, it takes in a direction (left or right) as a parameter, but you can change this if you'd like chooseWall : Autonomously choose which wall to follow (this can be left empty if you don't get to it) As always, start off by testing your code with the simulator before trying out the car itself.","title":"Wall Follower"},{"location":"curriculum/mod1/02_wall_follower/wall_follower/#competitive-lab-wall-following-drag-race","text":"Vroom Vroom","title":"Competitive Lab: Wall Following Drag Race"},{"location":"curriculum/mod1/02_wall_follower/wall_follower/#problem-statement","text":"It's time for your first challenge! You will navigate a racetrack using wall following. We will have two cars running simultaneously between two walls with \"lanes\" made of tape: We will time each team, and if you stray outside of your lane, you will get a time penalty (the lanes are about 3 cars wide, so it won't hopefully won't be too difficult).","title":"Problem Statement"},{"location":"curriculum/mod1/02_wall_follower/wall_follower/#idea-behind-the-algorithm","text":"Wall following is a fairly simple mode of autonomous navigation where your car will drive alongside a wall. There are multiple ways to do this. The simplest way would be to keep the car a certain distance from an obstacle to the side of the car (using a proportional or PID controller). Another strategy is to first find what is most likely to be a wall, then use it to predict the path of the wall. You could try to do a full linear regression, but this probably isn't the most efficient method. You can also do a simple \"bang-bang\" method and pick two random close points that seem to represent a wall, then have the car follow the line that passes through them.","title":"Idea Behind the Algorithm"},{"location":"curriculum/mod1/02_wall_follower/wall_follower/#implementation","text":"Here are some tips for a successful wall follower: Consider slicing the ranges data into more useful pieces. A majority of the data won\u2019t be useful to you if you only care about a wall to one side. When you can, try to use numpy operations rather than for loops in your code. Multidimensional slicing and broadcasting can make your code cleaner and much more efficient. You can turn any array into a numpy array with np.array , or you can integrate it directly with ROS like in this tutorial . Start off just looking for a wall on one side of the car; once this side works, implement it for the other side as well. You can map the left and right walls to different button (ex. 'A' activates the left wall follower, and 'B' activates the right wall follower). All the joystick buttons correspond to an integer in its ROS message; check the joystick callback function or run rostopic echo vesc/joy for hints on how to map buttons. Convert LIDAR points to cartesian points to calculate their resulting line. Make a separate function to do this. Be warned that if your algorithm only tries to make the car parallel to the wall, the car's average distance from the wall may develop cumulative error. Extra Challenges : Make the car autonomously decide which wall it should follow. You might base this off of factors like the distance or the accuracy of the predicted wall. If you also complete this, you could try to make your wall follower more robust, or try to implement a PID to make it smoother and more accurate. To keep your code organized and efficient, we've set up empty functions for you. I recommend only using drive_callback to set the driving speed and angle and putting everything else in other function. If there is too much code in a callback, it will become too slow and the car won't drive smoothly. Functions to write: convertPoint : convert polar points (points with only magnitude and angle) to cartesian coordinates estimateWall : Predict the wall using the method described above. Currently, it takes in a direction (left or right) as a parameter, but you can change this if you'd like chooseWall : Autonomously choose which wall to follow (this can be left empty if you don't get to it) As always, start off by testing your code with the simulator before trying out the car itself.","title":"Implementation"},{"location":"curriculum/mod1/03_safety_controller/safety_controller/","text":"Safety Controller To make sure we don't accidently break the cars, we're going to write a safety controller that will stop them from crashing. In racecar_ws/src/ , open safety.py . In the function is_safe , make the car stop if it detects anything 0.5 meters in front of it. Otherwise, let the current drive commands continue on. Remember that the actual car's LIDAR data takes 1081 rays, so it is indexed from 0-1080. The subscribers and publishers are already written for you, as well as the publish statement in the drive callback, so you won't have to worry about that. All of your code in controller.py (also in racecar_ws/src/ ) will go through the safety controller, so you can test your controller by driving there. Once you have this basic safety controller implemented, you can move on to moving your potential fields solution (maybe even your wall follower) into controller.py , which is where you will be writing most of your code on the car. Or if you'd like, you can make your safety controller more complex; for example, you could have it slow to a stop based on its current speed, or consider other LIDAR angles.","title":"Safety Controller"},{"location":"curriculum/mod1/03_safety_controller/safety_controller/#safety-controller","text":"To make sure we don't accidently break the cars, we're going to write a safety controller that will stop them from crashing. In racecar_ws/src/ , open safety.py . In the function is_safe , make the car stop if it detects anything 0.5 meters in front of it. Otherwise, let the current drive commands continue on. Remember that the actual car's LIDAR data takes 1081 rays, so it is indexed from 0-1080. The subscribers and publishers are already written for you, as well as the publish statement in the drive callback, so you won't have to worry about that. All of your code in controller.py (also in racecar_ws/src/ ) will go through the safety controller, so you can test your controller by driving there. Once you have this basic safety controller implemented, you can move on to moving your potential fields solution (maybe even your wall follower) into controller.py , which is where you will be writing most of your code on the car. Or if you'd like, you can make your safety controller more complex; for example, you could have it slow to a stop based on its current speed, or consider other LIDAR angles.","title":"Safety Controller"},{"location":"curriculum/mod1/04_potential_fields/potential_fields/","text":"Week 1 Final Challenge We've reached your first cumulative challenge! We'll be giving you a more complex racetrack to navigate, and you'll also have a new type of autonomous driving to explore called potential fields. How do potential fields work? Unlike wall following, potential fields takes in all the surrounding LIDAR data to decide how it should move. This means we now have the freedom to drive in any space, with or without walls! Let's imagine that every piece of LIDAR data has a vector pointing from the car to the point. In order for the car to avoid this obstacle, we want the car to consider this vector in the opposite direction so that it will move away from it. So, if it sees something 1 meter in front of it, the car will interpret this as a vector of length 1 meter in the backwards direction. In reality, the car will get a piece of data in every direction; we can then add up all of these vectors to create a final vector that will tell the car what direction it should move in and how fast it should go. In this diagram, there is also an attracting force (the goal), but we will only be using repelling forces (obstacles). You can just set a default speed and direction to account for this. For speed, we have to find a way to adjust for how far away a point is, but we want the opposite effect of proportional control; if an obstacle is closer to the car, we want it to take priority over other, farther away obstacles that we don't need to deal with at the moment. You can tell in the diagram that the vectors closest to the obstacle are larger. So if we have LIDAR input and car interpretations like this... The car should have a final force vector like this: We can implement this by adjusting each raw distance by some factor such that shorter distances produce much higher speeds and longer distances are slightly slower (think about what mathematical functions would give you a larger output for a smaller input). You also will want to multiple this value by some constant so that we get appropriate speeds for the car. Think about why we might prefer potential fields over wall following for this racetrack; if there are still walls, how could potential fields work better? Functions to write: * convertPoints : Convert points to cartesian coordinates * calcFinalVector : Calculate final drive vector * drive_callback : Publish the update speed and angle of the drive vector Currently, the final vector is of form [speed, angle]. You can change this if you'd like (ie if you want to change it to [x-speed, y-speed]) You can also implement a PID controller to try and make your driving more accurate. For now, copy the starter code into the wall_follower starter code so roslaunch will work correctly. You can either manuaully copy in the code or use cp ptFieldSim.py /your/path/wall_follower.py in your terminal/powershell.","title":"Potential Fields Controller"},{"location":"curriculum/mod1/04_potential_fields/potential_fields/#week-1-final-challenge","text":"We've reached your first cumulative challenge! We'll be giving you a more complex racetrack to navigate, and you'll also have a new type of autonomous driving to explore called potential fields.","title":"Week 1 Final Challenge"},{"location":"curriculum/mod1/04_potential_fields/potential_fields/#how-do-potential-fields-work","text":"Unlike wall following, potential fields takes in all the surrounding LIDAR data to decide how it should move. This means we now have the freedom to drive in any space, with or without walls! Let's imagine that every piece of LIDAR data has a vector pointing from the car to the point. In order for the car to avoid this obstacle, we want the car to consider this vector in the opposite direction so that it will move away from it. So, if it sees something 1 meter in front of it, the car will interpret this as a vector of length 1 meter in the backwards direction. In reality, the car will get a piece of data in every direction; we can then add up all of these vectors to create a final vector that will tell the car what direction it should move in and how fast it should go. In this diagram, there is also an attracting force (the goal), but we will only be using repelling forces (obstacles). You can just set a default speed and direction to account for this. For speed, we have to find a way to adjust for how far away a point is, but we want the opposite effect of proportional control; if an obstacle is closer to the car, we want it to take priority over other, farther away obstacles that we don't need to deal with at the moment. You can tell in the diagram that the vectors closest to the obstacle are larger. So if we have LIDAR input and car interpretations like this... The car should have a final force vector like this: We can implement this by adjusting each raw distance by some factor such that shorter distances produce much higher speeds and longer distances are slightly slower (think about what mathematical functions would give you a larger output for a smaller input). You also will want to multiple this value by some constant so that we get appropriate speeds for the car. Think about why we might prefer potential fields over wall following for this racetrack; if there are still walls, how could potential fields work better? Functions to write: * convertPoints : Convert points to cartesian coordinates * calcFinalVector : Calculate final drive vector * drive_callback : Publish the update speed and angle of the drive vector Currently, the final vector is of form [speed, angle]. You can change this if you'd like (ie if you want to change it to [x-speed, y-speed]) You can also implement a PID controller to try and make your driving more accurate. For now, copy the starter code into the wall_follower starter code so roslaunch will work correctly. You can either manuaully copy in the code or use cp ptFieldSim.py /your/path/wall_follower.py in your terminal/powershell.","title":"How do potential fields work?"},{"location":"curriculum/mod2/01_segmentation/segmentation/","text":"Color Segmentation and openCV A color space is an organization of colors typically used to encode an image. The most common color space that most people may have heard of is the RGB color space. Others include the CMYK and YIQ models, but a common color space for robotics is the HSV color space. HSV stands for H ue, S aturation, and V alue. Saturation is the intensity of the color. Hue is which one of the standard \"colors\" this particular color most resembles (we count starting from red to yellow, green, cyan, blue, and end with magenta). Value is the brightness of the color, where 0 is completely black and 100 is no black. Goals: Accurately guess what HSV filter is being used Determine the threshold values for detecting a green flag and a cone Code your racecar to stop when it sees an orange cone at a desired distance away Part 0: Setup You can download the files you need from this link . You will be working mainly with slider_colorRecognition.py , challenge.py , and driveNode.py . If you are not a Linux user, you will need to have openCV version 4 installed onto your laptop. To check which version you have/if you have a version all, go into terminal and type python import cv2 cv2.__version__ If you have the wrong version, first uninstall opencv-python by typing pip uninstall opencv-python . To install the correct version, type pip install opencv-python==4.1.0.25 . You will also need to split up into two teams within your group for parts 1 and 2; when picking these sub-teams, make sure one person in each team has a working laptop camera. If no one in your team has working/good cameras, talk to your TA to see if there are extra webcameras you could borrow. Copy the files over for this lab and put newZed.py, driveNode.py, and color_segmentation.py into the scripts folder inside racecar_ws/src in your robot . Part 1: Correctly Identify Color Filters as well as Hue, Value, and Saturation We'll be working with your laptop only for this part of the lab. Run challenge.py , which will apply a random filter to this image: Note: If you get an assertion error / error -215, the image pathing is wrong! Make sure challenge.py is in the same directory as your resources folder since the code calls for \"resources/rainbow.jpg\" Once challenge completely executes, you can press 'z' to randomly select from components of RGB and HSV: red, blue, green; hue, saturation, value. The program will modify the image to only show the values of the component that was picked. Work with your team to figure out which image is showing which component, then click the dropdown and compare. If you got any wrong, discuss with your team and call over a TA to help clarify any questions. Answers: ![Red](resources/red.jpg) ![Green](resources/green.jpg) ![Blue](resources/blue.jpg) ![Saturation](resources/saturation.jpg) ![Hue](resources/hue.jpg) ![Value](resources/value.jpg) Part 2: Detecting Objects Using Color Segmentation Color segmentation is an algorithm used to find specific objects. We first specify the upper and lower bounds for the components of whatever color space we are using ie. hue, saturation, value for HSV and so on. The algorithm then takes in an image and applies a filter to it that \"erases\" any pixel that does not fall within the specified range, allowing the camera to focus on specific objects. Note: When you run slider_colorSegmentation, you must add a source flag. You do this by typing in the command to run the program normally in your terminal (python slider_colorRecognition.py) then typing -s and a source number (0 for your laptop's camera and 1 for a usb camera). For example, a person using their laptop's camera would go into their terminal and type python slider_colorSegmentation.py -s 0 Look at your objects and make note of which component (hue, saturation, value) will be most useful for you Run slider_colorSegmentation.py. This gives you three separate windows, one containing the camera output, one with the filter, and one with the sliders. In order to exit the program, press and hold the escape key. If you are missing the third slider window, try looking at your open windows and selecting it. You will see two sliders for each component of the color space. One is the lower bound and the other is the upper bound. The area in between is the area that the algorithm will target; everything else will be excluded. In the first window, a green bounding box will be drawn around the largest object within the bounds you have specified. Figure out what the correct bounds are for each object and take notes! Try to get the most accurate filter (in other words, only the specific object you want shows up on-screen). Write down your cone hsv ranges! Note: Are HSV values always constant? What things could affect what a camera sees? Make sure you think about these things and how they could affect what the robot sees. These are the values that worked for us: low_range = np.array( [ 0, 220, 161] ) high_range = np.array( [28, 255, 255] ) Compare, do yours look similar or different? Do yours work better? Part 3: Drive/Stop with Cones Join back up with your bigger team since you are working with the car now! In this part of the lab, we will be programming the car to stop when it sees an orange cone (Look at your teammate's clothing to check if they're wearing these colors!). We will first check the HSV values of the color we'll be using and take note of these values, then mess with driveNode.py to make the car stop when it sees the orange cone at a certain distance away. Open up slider_colorSegmentation.py and identify the upper and lower bounds of the HSV space necessary to identify the color of the cone you are working with. Once you have the numbers for the bounds, go into color_segmentation.py and add these values in the marked area. This ensures that the filter is applied to the images traveling through ROS. Now, we will be working with driveNode.py , which is where the car will take in the filtered image and publish drive commands. In color_segmentation.py , there is a function called cd_color_segmentation(), which takes in an image, applies a filter, and finds the biggest viable object. It then draws a box around the object and returns the box's coordinates, in the form of a nested tuple ( (xL,yL), (xR,yR) ) . xL and yL are the x,y coordinates of the upper left corner of the box while xR and yR are the x,y coordinates of the bottom right corner of the box. You will be working with two functions: size_calc() and drive(). size_calc() : this function will primarily modify the class's self.box_size parameter. Using the information you have been given, find the total size/area of the box given by cd_color_segmentation(). drive() : this function is called after size_calc() and when the racecar is not in autonomous mode. drive() tells the robot how to move and what to do when it sees the cone. Your task will be to have the robot stop only when it is 1 meter or less away from the cone and drive forward otherwise. You don't have to worry about publishing in this section since everything is published in the main loop (see main() ), you just have to update the car's speed. To test your code, have teleop, safety, and zed running. Then, in the scripts folder run driveNode.py like normal(type python driveNode.py). This should run your new driving node. Challenge: Once you have completed all the parts, try making the car stop when the cone is a certain distance away and in the center of the robot's view!","title":"Color Segmentation"},{"location":"curriculum/mod2/01_segmentation/segmentation/#color-segmentation-and-opencv","text":"A color space is an organization of colors typically used to encode an image. The most common color space that most people may have heard of is the RGB color space. Others include the CMYK and YIQ models, but a common color space for robotics is the HSV color space. HSV stands for H ue, S aturation, and V alue. Saturation is the intensity of the color. Hue is which one of the standard \"colors\" this particular color most resembles (we count starting from red to yellow, green, cyan, blue, and end with magenta). Value is the brightness of the color, where 0 is completely black and 100 is no black.","title":"Color Segmentation and openCV"},{"location":"curriculum/mod2/01_segmentation/segmentation/#goals","text":"Accurately guess what HSV filter is being used Determine the threshold values for detecting a green flag and a cone Code your racecar to stop when it sees an orange cone at a desired distance away","title":"Goals:"},{"location":"curriculum/mod2/01_segmentation/segmentation/#part-0-setup","text":"You can download the files you need from this link . You will be working mainly with slider_colorRecognition.py , challenge.py , and driveNode.py . If you are not a Linux user, you will need to have openCV version 4 installed onto your laptop. To check which version you have/if you have a version all, go into terminal and type python import cv2 cv2.__version__ If you have the wrong version, first uninstall opencv-python by typing pip uninstall opencv-python . To install the correct version, type pip install opencv-python==4.1.0.25 . You will also need to split up into two teams within your group for parts 1 and 2; when picking these sub-teams, make sure one person in each team has a working laptop camera. If no one in your team has working/good cameras, talk to your TA to see if there are extra webcameras you could borrow. Copy the files over for this lab and put newZed.py, driveNode.py, and color_segmentation.py into the scripts folder inside racecar_ws/src in your robot .","title":"Part 0: Setup"},{"location":"curriculum/mod2/01_segmentation/segmentation/#part-1-correctly-identify-color-filters-as-well-as-hue-value-and-saturation","text":"We'll be working with your laptop only for this part of the lab. Run challenge.py , which will apply a random filter to this image: Note: If you get an assertion error / error -215, the image pathing is wrong! Make sure challenge.py is in the same directory as your resources folder since the code calls for \"resources/rainbow.jpg\" Once challenge completely executes, you can press 'z' to randomly select from components of RGB and HSV: red, blue, green; hue, saturation, value. The program will modify the image to only show the values of the component that was picked. Work with your team to figure out which image is showing which component, then click the dropdown and compare. If you got any wrong, discuss with your team and call over a TA to help clarify any questions. Answers: ![Red](resources/red.jpg) ![Green](resources/green.jpg) ![Blue](resources/blue.jpg) ![Saturation](resources/saturation.jpg) ![Hue](resources/hue.jpg) ![Value](resources/value.jpg)","title":"Part 1: Correctly Identify Color Filters as well as Hue, Value, and Saturation"},{"location":"curriculum/mod2/01_segmentation/segmentation/#part-2-detecting-objects-using-color-segmentation","text":"Color segmentation is an algorithm used to find specific objects. We first specify the upper and lower bounds for the components of whatever color space we are using ie. hue, saturation, value for HSV and so on. The algorithm then takes in an image and applies a filter to it that \"erases\" any pixel that does not fall within the specified range, allowing the camera to focus on specific objects. Note: When you run slider_colorSegmentation, you must add a source flag. You do this by typing in the command to run the program normally in your terminal (python slider_colorRecognition.py) then typing -s and a source number (0 for your laptop's camera and 1 for a usb camera). For example, a person using their laptop's camera would go into their terminal and type python slider_colorSegmentation.py -s 0 Look at your objects and make note of which component (hue, saturation, value) will be most useful for you Run slider_colorSegmentation.py. This gives you three separate windows, one containing the camera output, one with the filter, and one with the sliders. In order to exit the program, press and hold the escape key. If you are missing the third slider window, try looking at your open windows and selecting it. You will see two sliders for each component of the color space. One is the lower bound and the other is the upper bound. The area in between is the area that the algorithm will target; everything else will be excluded. In the first window, a green bounding box will be drawn around the largest object within the bounds you have specified. Figure out what the correct bounds are for each object and take notes! Try to get the most accurate filter (in other words, only the specific object you want shows up on-screen). Write down your cone hsv ranges! Note: Are HSV values always constant? What things could affect what a camera sees? Make sure you think about these things and how they could affect what the robot sees. These are the values that worked for us: low_range = np.array( [ 0, 220, 161] ) high_range = np.array( [28, 255, 255] ) Compare, do yours look similar or different? Do yours work better?","title":"Part 2: Detecting Objects Using Color Segmentation"},{"location":"curriculum/mod2/01_segmentation/segmentation/#part-3-drivestop-with-cones","text":"Join back up with your bigger team since you are working with the car now! In this part of the lab, we will be programming the car to stop when it sees an orange cone (Look at your teammate's clothing to check if they're wearing these colors!). We will first check the HSV values of the color we'll be using and take note of these values, then mess with driveNode.py to make the car stop when it sees the orange cone at a certain distance away. Open up slider_colorSegmentation.py and identify the upper and lower bounds of the HSV space necessary to identify the color of the cone you are working with. Once you have the numbers for the bounds, go into color_segmentation.py and add these values in the marked area. This ensures that the filter is applied to the images traveling through ROS. Now, we will be working with driveNode.py , which is where the car will take in the filtered image and publish drive commands. In color_segmentation.py , there is a function called cd_color_segmentation(), which takes in an image, applies a filter, and finds the biggest viable object. It then draws a box around the object and returns the box's coordinates, in the form of a nested tuple ( (xL,yL), (xR,yR) ) . xL and yL are the x,y coordinates of the upper left corner of the box while xR and yR are the x,y coordinates of the bottom right corner of the box. You will be working with two functions: size_calc() and drive(). size_calc() : this function will primarily modify the class's self.box_size parameter. Using the information you have been given, find the total size/area of the box given by cd_color_segmentation(). drive() : this function is called after size_calc() and when the racecar is not in autonomous mode. drive() tells the robot how to move and what to do when it sees the cone. Your task will be to have the robot stop only when it is 1 meter or less away from the cone and drive forward otherwise. You don't have to worry about publishing in this section since everything is published in the main loop (see main() ), you just have to update the car's speed. To test your code, have teleop, safety, and zed running. Then, in the scripts folder run driveNode.py like normal(type python driveNode.py). This should run your new driving node. Challenge: Once you have completed all the parts, try making the car stop when the cone is a certain distance away and in the center of the robot's view!","title":"Part 3: Drive/Stop with Cones"},{"location":"curriculum/mod2/02_line_follow/line_follow/","text":"End of Week Challenge: Line Follow For this week's final challenge, your team will have to program your racecar to get to the finish line of the course by following the colored tape laid down on the ground. Your car needs to do two things: Recognize where the line of your choice is compared to other lines Follow the target line realiably Reach the end without human intervention Here is a link to a blank copy of color_segmentation and driveNode. driveNode and color_segmentation have been modified so that it will publish the image from color_segmentation to the topic IMAGE_TOPIC (\"/zed/zed_node/color_seg_output\"). Log into docker using the same command you were using before but after fishberg/racecar, add a space and your car number. You will then be able to open rqt_image_view in docker and choose the color_seg_output topic to view the zed camera feed after the color_seg filter. If you get a \"connection invalid\" error, ignore it , it tends to work anyways. Apart from the publisher thing that was added, these are basically blank files ; feel free to mess around with them as you wish.","title":"Line Following"},{"location":"curriculum/mod2/02_line_follow/line_follow/#end-of-week-challenge-line-follow","text":"For this week's final challenge, your team will have to program your racecar to get to the finish line of the course by following the colored tape laid down on the ground. Your car needs to do two things: Recognize where the line of your choice is compared to other lines Follow the target line realiably Reach the end without human intervention Here is a link to a blank copy of color_segmentation and driveNode. driveNode and color_segmentation have been modified so that it will publish the image from color_segmentation to the topic IMAGE_TOPIC (\"/zed/zed_node/color_seg_output\"). Log into docker using the same command you were using before but after fishberg/racecar, add a space and your car number. You will then be able to open rqt_image_view in docker and choose the color_seg_output topic to view the zed camera feed after the color_seg filter. If you get a \"connection invalid\" error, ignore it , it tends to work anyways. Apart from the publisher thing that was added, these are basically blank files ; feel free to mess around with them as you wish.","title":"End of Week Challenge: Line Follow"},{"location":"curriculum/mod2/03_feature_detect/feature_detect/","text":"Midweek Challenge: Cone and Sign Parking For this week's first challenge, we'll simulate your car driving down a street, turning into a parking lot, and parking in a designated spot. Your car will have to do two things: Read the direction of the sign and turn in that direction Spot the orange cone and park in front of it These two things can be developed independently of each other; our recommendation is that half of your team tackles the first task and the other tackles the second task. You can then combine the code into one file. Writing the sign recognition code will require reading some documentation, writing some openCV, and heavy on python, while the parking will deal with an extension of the drive-stop lab and state machines. Sign Detection For sign detection, we will be using feature detection in the form of the ORB (Scale-Invariant Feature Transform) algorithm and color detection. This code is only designed to run on your laptop; once you can verify that it works on your laptop you can keep moving forward. The feature detection part of the program is the ORB section; this code uses FAST to detect keypoints within the template image and compares those to the keypoints of the camera image, drawing a box around the sub-image with the most matches. This is good and all but a right one-way sign does not have that many different keypoints compared to a left one-way sign, which means that ORB will not be able to tell the difference between the two signs. This is where you will have to implement an algorithm that can distinguish between the two now that you have narrowed the image to just the sign using ORB (crop the camera image to just the box around the sign). Here are two approaches: Template matching: Takes a template image and \"slides\" it over a given image, comparing the pixel values at every point. It then remembers the point with the highest match value. There are plenty of online tutorials that can show you how to write this type of algorithm. Pixel counting: A right one-way sign will have more white pixels in the right-most side of the sign that a left one-way sign and vice-versa. Since you have narrowed down the camera image to just the sign using ORB, you can iterate through the pixels of the sign and count the number of white pixels. There are a couple ways to do this, including converting your target image to grayscale and using the cv2.countNonZero() function. In order for template matching to work, the sign needs to be approached at the same angle as your template image and traditionally has to be the same size (how can this be worked around?) while pixel counting is less consistent and could be thrown off more easily. Its up to you to implement an algorithm that fits your situation. Here is an example run of turnRectStarter: python turnRectStarter.py -i ./images/oneway.jpg -l \"One-Way-Sign\" -s 0 -i tells the program what image to get -l is a lowercase l that tells what to label the detected object -s tells them the source of camera, just like slider_colorSegmentation.py Here is what our orb_det returns when seeing a right way sign: Here is what our orb_det returns when seeing a left way sign: Changing turnRectStarter.py and driveNode.py to work with ROS The images from the Zed camera and openCV images sadly do not work too well with each other. That means we will have to change some stuff so that it works with ROS. First, we have to get rid of the cv2 source code inside turnRectStarter.py (the code that takes in images from the camera). We can delete any cv2.VideoCapture() function within the code. You can also delete any part that has the cam.read() function (including the loop it is in) and set \"frame\" to be a passed-in image (that means you need to alter the function to take in an extra parameter \"source\") Then, you can delete any cv2.imshow(), cam.release, cv2.destroyAllWindows(), and cv2.waitKey(), since ssh'ing does not allow these windows to pop up. You will need to delete the while loop and need to pop out all the code within the loop (ie change the indentation to be in-line with the outside code) Change any \"continue\" to a pass and comment out the parts with ap (such as ap.argparse, ap.add_argument, etc.) Then, delete the if main : part of the lab (bottom two lines) Go to the line that has label and remove the if/else statement. Then, remove the cv2.putText() line since the robot really doesn't need that Finally, make a variable for your two bounding boxes, which the function will now return. Find the two instances of cv2.rectangle(), which will key you in to what the bounding box coordinates are and where they are stored, and set your bounding box variable equal to these coordinates We will now look at driveNode.py to see how the images are passed in. Go into your init function and find the self.camera_data variable. That variable holds the camera data from the Zed camera. Looking further down at the callback function, you can see that cd_color_segmentation takes in self.camera_data.cv_image. This is the Zed image that has been molded to work with openCV, so that is what you will be passing into orb_det() as your source image. Notice that orb_det takes in an image apart from the source(the one way sign image in this case) so make sure you pass in the location of the one-way image into the orb_det function. If you get confused, the implementation of the orb_det function should be very similar to the implementation of the cd_color_segmentation function except for the extra \"comparison\" image. Parking Your task is to have the car drive up to the cone and park in front of it! You will need to build upon the code in driveNode.py since parking is just an extension of drive-stop. As you write your code, make sure you START SMALL and SIMPLE as a sanity check. If your simple code works, you can build up to more complicated things. You should try to get the cone to sit between the car's wheels and have the car stop within a meter of the cone; penalties will be given for failing to satisfy these conditions. Once you finish this, think about how your robot will respond to the sign detection. Your teammates are working on reading the sign and returning a direction; what will your robot do once that direction has been sent back? Develop the system necessary to make your robot turn in the desired direction consistently.","title":"Feature Detection"},{"location":"curriculum/mod2/03_feature_detect/feature_detect/#midweek-challenge-cone-and-sign-parking","text":"For this week's first challenge, we'll simulate your car driving down a street, turning into a parking lot, and parking in a designated spot. Your car will have to do two things: Read the direction of the sign and turn in that direction Spot the orange cone and park in front of it These two things can be developed independently of each other; our recommendation is that half of your team tackles the first task and the other tackles the second task. You can then combine the code into one file. Writing the sign recognition code will require reading some documentation, writing some openCV, and heavy on python, while the parking will deal with an extension of the drive-stop lab and state machines.","title":"Midweek Challenge: Cone and Sign Parking"},{"location":"curriculum/mod2/03_feature_detect/feature_detect/#sign-detection","text":"For sign detection, we will be using feature detection in the form of the ORB (Scale-Invariant Feature Transform) algorithm and color detection. This code is only designed to run on your laptop; once you can verify that it works on your laptop you can keep moving forward. The feature detection part of the program is the ORB section; this code uses FAST to detect keypoints within the template image and compares those to the keypoints of the camera image, drawing a box around the sub-image with the most matches. This is good and all but a right one-way sign does not have that many different keypoints compared to a left one-way sign, which means that ORB will not be able to tell the difference between the two signs. This is where you will have to implement an algorithm that can distinguish between the two now that you have narrowed the image to just the sign using ORB (crop the camera image to just the box around the sign). Here are two approaches: Template matching: Takes a template image and \"slides\" it over a given image, comparing the pixel values at every point. It then remembers the point with the highest match value. There are plenty of online tutorials that can show you how to write this type of algorithm. Pixel counting: A right one-way sign will have more white pixels in the right-most side of the sign that a left one-way sign and vice-versa. Since you have narrowed down the camera image to just the sign using ORB, you can iterate through the pixels of the sign and count the number of white pixels. There are a couple ways to do this, including converting your target image to grayscale and using the cv2.countNonZero() function. In order for template matching to work, the sign needs to be approached at the same angle as your template image and traditionally has to be the same size (how can this be worked around?) while pixel counting is less consistent and could be thrown off more easily. Its up to you to implement an algorithm that fits your situation. Here is an example run of turnRectStarter: python turnRectStarter.py -i ./images/oneway.jpg -l \"One-Way-Sign\" -s 0 -i tells the program what image to get -l is a lowercase l that tells what to label the detected object -s tells them the source of camera, just like slider_colorSegmentation.py Here is what our orb_det returns when seeing a right way sign: Here is what our orb_det returns when seeing a left way sign:","title":"Sign Detection"},{"location":"curriculum/mod2/03_feature_detect/feature_detect/#changing-turnrectstarterpy-and-drivenodepy-to-work-with-ros","text":"The images from the Zed camera and openCV images sadly do not work too well with each other. That means we will have to change some stuff so that it works with ROS. First, we have to get rid of the cv2 source code inside turnRectStarter.py (the code that takes in images from the camera). We can delete any cv2.VideoCapture() function within the code. You can also delete any part that has the cam.read() function (including the loop it is in) and set \"frame\" to be a passed-in image (that means you need to alter the function to take in an extra parameter \"source\") Then, you can delete any cv2.imshow(), cam.release, cv2.destroyAllWindows(), and cv2.waitKey(), since ssh'ing does not allow these windows to pop up. You will need to delete the while loop and need to pop out all the code within the loop (ie change the indentation to be in-line with the outside code) Change any \"continue\" to a pass and comment out the parts with ap (such as ap.argparse, ap.add_argument, etc.) Then, delete the if main : part of the lab (bottom two lines) Go to the line that has label and remove the if/else statement. Then, remove the cv2.putText() line since the robot really doesn't need that Finally, make a variable for your two bounding boxes, which the function will now return. Find the two instances of cv2.rectangle(), which will key you in to what the bounding box coordinates are and where they are stored, and set your bounding box variable equal to these coordinates We will now look at driveNode.py to see how the images are passed in. Go into your init function and find the self.camera_data variable. That variable holds the camera data from the Zed camera. Looking further down at the callback function, you can see that cd_color_segmentation takes in self.camera_data.cv_image. This is the Zed image that has been molded to work with openCV, so that is what you will be passing into orb_det() as your source image. Notice that orb_det takes in an image apart from the source(the one way sign image in this case) so make sure you pass in the location of the one-way image into the orb_det function. If you get confused, the implementation of the orb_det function should be very similar to the implementation of the cd_color_segmentation function except for the extra \"comparison\" image.","title":"Changing turnRectStarter.py and driveNode.py to work with ROS"},{"location":"curriculum/mod2/03_feature_detect/feature_detect/#parking","text":"Your task is to have the car drive up to the cone and park in front of it! You will need to build upon the code in driveNode.py since parking is just an extension of drive-stop. As you write your code, make sure you START SMALL and SIMPLE as a sanity check. If your simple code works, you can build up to more complicated things. You should try to get the cone to sit between the car's wheels and have the car stop within a meter of the cone; penalties will be given for failing to satisfy these conditions. Once you finish this, think about how your robot will respond to the sign detection. Your teammates are working on reading the sign and returning a direction; what will your robot do once that direction has been sent back? Develop the system necessary to make your robot turn in the desired direction consistently.","title":"Parking"},{"location":"curriculum/mod3/01_ar_sounds/ar_sounds/","text":"AR Tags and Sound Today, you will be learning to change states based off of reading AR tags and using your speakers to indicate a state change. Goals Implement a state machine Get your car to read AR tags Produce sounds when the car changes states AR Tags AR tags are essentially less complex QR codes, from which our cars can read a corresponding number for each unique tag as well as the position and orientation of the tag. While this can be useful for localization, for now we will just be using it to change states based on which AR tag it sees. To be able to do this, we have to install a ROS package that interprets AR tags: In order to install this library, you need to enter the following code into the command line (make sure you are outside of the racecar_ws for this): sudo apt-get install ros-melodic-ar-track-alvar . Next, there are several topics you need to change in a launch file to configure the wrapper to our car. Navigate to the folder with all the package's launch files: cd /opt/ros/melodic/share/ar_track_alvar/launch . Enter: sudo cp pr2_indiv_no_kinect.launch racecar_ar.launch . Then, sudo chmod a+rwx racecar_ar.launch to change the read/write permissions. Now, use vim to edit racecar_ar.launch to modify the following topics (listed after default= in each line): For \"cam_image_topic\", change it to \"/zed/zed_node/rgb/image_rect_color\" For \"cam_info_topic\", change it to \"/zed/zed_node/rgb/camera_info\" For \"output_frame\", change it to \"/base_link\" It should look like this: To launch the AR node, you can type roslaunch ar_track_alvar racecar_ar.launch . You will have to run this whenever you want to read AR tags. While the ar_track_alvar node is running, whenever the car sees an AR tag with the camera, it will publish the AR tag's ID (its corresponding number) and its physical properties to ar_pose_marker . So, we have to write a new subscriber for this topic! The object that it will receive is of type AlvarMarkers . The markers attribute is a list of all the AR tags it sees, and the id attribute of each index will have the AR tag's unique number that you will need to access for this lab. For example, if you wanted the ID number of the first AR tag available and you have the callback's parameter tags , you could call tags.markers[0].id . Now, you are going to have your car change states based on what AR tag ID it sees. You can write this in your AR tag callback. There will be six commands (for six AR tags): forwards, backwards, stop, left, right, and straight. Note that the car will detect multiple AR tags at once and has to respond accordingly. For example, if it sees both forwards and left, it should make a left turn while going forwards. You won't ever have to respond to multiple speed-related tags or turn-related tags at the same time (so you don't need to worry about seeing both a forwards and backwards tag). The AR tags IDs correspond to the following commands: 0: Forward 1: Backward 2: Stop 3: Right 4: Left 5: Straight This time, all we're giving you in the starter code ( driveAr.py ) are the basic ROS things (publishers/subscribers, publishing, etc) and a basic framework; everything concerning the driving, reading AR tags, and setting states states is up to you. You should write everything having to do with the AR tags in the arCallback function, and set your driving speed and steering angle in the driveCallback function. Download the code here . Using Sounds We've given you speakers for your cars, so now we will be able to play sounds! This is helpful for debugging and knowing when you've changed states, since printing can sometimes slow the car down. After you plug in the speakers, we have to make sure the car outputs to these speakers instead of the default. In a terminal that ssh'd into your car, type in pactl list short sinks . One entry should have a name similar to alsa_output.usb-Generic_USB2.0_Device_20130100ph0-00.iec958-stereo . Copy this name and type pactl set-default-sink name_of_your_speaker_this_is_not_the_command . Now we have to change the volume: enter amixer -D pulse sset Master 60% , where the percentage is the volume (please be reasonable with the volume, we will come after you if your car is too loud). In driveAr.py , you can publish the current state to the sound node using sound_pub (you must write the publish commands for this). Every time you change states, it should play a sound once . In soundNode.py , there is an empty dictionary. The key will be a string with the name of the state you pass in through the publisher, and the value is a string of the path of the sound file you want to use (for example, the value for the sound file tone1 would be \"./pythonSounds/tone1.wav\" ). I would recommend using left.wav and right.wav when the car turns, and different tones for the speed controls and straight.","title":"AR Tags & Sounds"},{"location":"curriculum/mod3/01_ar_sounds/ar_sounds/#ar-tags-and-sound","text":"Today, you will be learning to change states based off of reading AR tags and using your speakers to indicate a state change.","title":"AR Tags and Sound"},{"location":"curriculum/mod3/01_ar_sounds/ar_sounds/#goals","text":"Implement a state machine Get your car to read AR tags Produce sounds when the car changes states","title":"Goals"},{"location":"curriculum/mod3/01_ar_sounds/ar_sounds/#ar-tags","text":"AR tags are essentially less complex QR codes, from which our cars can read a corresponding number for each unique tag as well as the position and orientation of the tag. While this can be useful for localization, for now we will just be using it to change states based on which AR tag it sees. To be able to do this, we have to install a ROS package that interprets AR tags: In order to install this library, you need to enter the following code into the command line (make sure you are outside of the racecar_ws for this): sudo apt-get install ros-melodic-ar-track-alvar . Next, there are several topics you need to change in a launch file to configure the wrapper to our car. Navigate to the folder with all the package's launch files: cd /opt/ros/melodic/share/ar_track_alvar/launch . Enter: sudo cp pr2_indiv_no_kinect.launch racecar_ar.launch . Then, sudo chmod a+rwx racecar_ar.launch to change the read/write permissions. Now, use vim to edit racecar_ar.launch to modify the following topics (listed after default= in each line): For \"cam_image_topic\", change it to \"/zed/zed_node/rgb/image_rect_color\" For \"cam_info_topic\", change it to \"/zed/zed_node/rgb/camera_info\" For \"output_frame\", change it to \"/base_link\" It should look like this: To launch the AR node, you can type roslaunch ar_track_alvar racecar_ar.launch . You will have to run this whenever you want to read AR tags. While the ar_track_alvar node is running, whenever the car sees an AR tag with the camera, it will publish the AR tag's ID (its corresponding number) and its physical properties to ar_pose_marker . So, we have to write a new subscriber for this topic! The object that it will receive is of type AlvarMarkers . The markers attribute is a list of all the AR tags it sees, and the id attribute of each index will have the AR tag's unique number that you will need to access for this lab. For example, if you wanted the ID number of the first AR tag available and you have the callback's parameter tags , you could call tags.markers[0].id . Now, you are going to have your car change states based on what AR tag ID it sees. You can write this in your AR tag callback. There will be six commands (for six AR tags): forwards, backwards, stop, left, right, and straight. Note that the car will detect multiple AR tags at once and has to respond accordingly. For example, if it sees both forwards and left, it should make a left turn while going forwards. You won't ever have to respond to multiple speed-related tags or turn-related tags at the same time (so you don't need to worry about seeing both a forwards and backwards tag). The AR tags IDs correspond to the following commands: 0: Forward 1: Backward 2: Stop 3: Right 4: Left 5: Straight This time, all we're giving you in the starter code ( driveAr.py ) are the basic ROS things (publishers/subscribers, publishing, etc) and a basic framework; everything concerning the driving, reading AR tags, and setting states states is up to you. You should write everything having to do with the AR tags in the arCallback function, and set your driving speed and steering angle in the driveCallback function. Download the code here .","title":"AR Tags"},{"location":"curriculum/mod3/01_ar_sounds/ar_sounds/#using-sounds","text":"We've given you speakers for your cars, so now we will be able to play sounds! This is helpful for debugging and knowing when you've changed states, since printing can sometimes slow the car down. After you plug in the speakers, we have to make sure the car outputs to these speakers instead of the default. In a terminal that ssh'd into your car, type in pactl list short sinks . One entry should have a name similar to alsa_output.usb-Generic_USB2.0_Device_20130100ph0-00.iec958-stereo . Copy this name and type pactl set-default-sink name_of_your_speaker_this_is_not_the_command . Now we have to change the volume: enter amixer -D pulse sset Master 60% , where the percentage is the volume (please be reasonable with the volume, we will come after you if your car is too loud). In driveAr.py , you can publish the current state to the sound node using sound_pub (you must write the publish commands for this). Every time you change states, it should play a sound once . In soundNode.py , there is an empty dictionary. The key will be a string with the name of the state you pass in through the publisher, and the value is a string of the path of the sound file you want to use (for example, the value for the sound file tone1 would be \"./pythonSounds/tone1.wav\" ). I would recommend using left.wav and right.wav when the car turns, and different tones for the speed controls and straight.","title":"Using Sounds"},{"location":"curriculum/mod3/02_maps_bags/maps_bags/","text":"Rideshare Challenge For this challenge, you are going to mimic a rideshare program with your car! There will be two sections in the track: a \"pickup\" area and a \"drop-off\" area. The pickup area will have an AR tag that will correspond to a set location in the drop-off area. You will have to localize using your map to reach the correct drop-off location. Once you reach your goal, you should return to the pickup area, where you will read a different tag and start the process again. This means that you will have to set both the drop-off location as a goal, as well as the starting location (the AR tag will always be in the same place). You are picking up three \"customers\": 1, 2, and 5 (this is the number the AR tag returns when read). These will be randomly given to you at the pickup point (one at a time) and they have to be returned to their \"houses\". The order of the houses, from left to right, is 1 -> 2 -> 5 (2 is the corner spot). After you have dropped off the last customer, have your robot return to the pickup spot and stop. The topic you will need for the lab is /pf/viz/inferred_pose . There will also be obstacles in the track, so you will have to integrate your Potential Field control as well. THERE IS NO STARTER CODE FOR THIS LAB - flex those coding muscles! \u1559\u3033 \u0298 \u2013 \u0298 \u3035\u1557 Securing the Bag Navigate to the Installation: Google Cartographer on our website and follow the new instructions there. Once you have done this, you will need to record a map. You can find instructions for how to do this in References , in the Cartographer section under \"Running off of a Rosbag\" . After you have done this, read through the Particle Filter section of References for instructions on how to work with the maps you have made (like looking at the relative X and Y values of your current position).","title":"Maps & Rosbags"},{"location":"curriculum/mod3/02_maps_bags/maps_bags/#rideshare-challenge","text":"For this challenge, you are going to mimic a rideshare program with your car! There will be two sections in the track: a \"pickup\" area and a \"drop-off\" area. The pickup area will have an AR tag that will correspond to a set location in the drop-off area. You will have to localize using your map to reach the correct drop-off location. Once you reach your goal, you should return to the pickup area, where you will read a different tag and start the process again. This means that you will have to set both the drop-off location as a goal, as well as the starting location (the AR tag will always be in the same place). You are picking up three \"customers\": 1, 2, and 5 (this is the number the AR tag returns when read). These will be randomly given to you at the pickup point (one at a time) and they have to be returned to their \"houses\". The order of the houses, from left to right, is 1 -> 2 -> 5 (2 is the corner spot). After you have dropped off the last customer, have your robot return to the pickup spot and stop. The topic you will need for the lab is /pf/viz/inferred_pose . There will also be obstacles in the track, so you will have to integrate your Potential Field control as well. THERE IS NO STARTER CODE FOR THIS LAB - flex those coding muscles! \u1559\u3033 \u0298 \u2013 \u0298 \u3035\u1557","title":"Rideshare Challenge"},{"location":"curriculum/mod3/02_maps_bags/maps_bags/#securing-the-bag","text":"Navigate to the Installation: Google Cartographer on our website and follow the new instructions there. Once you have done this, you will need to record a map. You can find instructions for how to do this in References , in the Cartographer section under \"Running off of a Rosbag\" . After you have done this, read through the Particle Filter section of References for instructions on how to work with the maps you have made (like looking at the relative X and Y values of your current position).","title":"Securing the Bag"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/","text":"Warning Be sure to FIRST focus on the practice racetracks set up (if this is done during Week 4, before the Grand Final). Solely machine learning based object detection will NOT get a team through to the end. Ensure your team has a conceptual and practical plan of attack for the final course, and a backup implementation of the sign detection (we know most of you will try to detect signs with this model). Histogram of Oriented Gradients (HOG) If interested in the entire background of the algorithm, the paper is here . This algorithm extracts key features from an image or set of images. These images are generally pre-processed to normalize lighting and shadows. It then finds features based on the most extreme gradient changes between pixels, which are then grouped into cells for an aggregate weight and direction. Intuitively, these features are strongest around the edges of objects and pixel intensity changes. An example HOG descriptor : The original sign The computed descriptor Support Vector Machines (SVM) This is a supervised machine learning method, depending on labeled training data. Based on the provided data, an SVM tries to find an optimal hyperplane (based on user parameters) to separate data points into unique classes. Optimal, in this case, is defined by separating clusters of data while maximizing the distance from those points - known as maximizing the margin of the training data. The hyperplane is built in N-dimensional space, and our model will be staged in 2-D or 3-D space, depending on parameters. A classic linear SVM: A nonlinear separable SVM, when taken to a higher dimension: HOG-SVM Interaction The data points are feature vectors in our case. The feature vectors built by the HOG are fed into the SVM, where they are separated into classes of feature vectors. More to come. Directory Setup Download this zip file here on Google Drive onto your computer. After a full setup, your folder directory should look like this: | - hog-svm |-TrainHOG.cpp |-detector.py |-batchRename.py |-feedSaver.py |-howToBuild.txt |-makefile |-positiveTrainingImages |-[your images] |-negativeTrainingImages |-[your images] |-processedPosTrainingImages |-[cropped and renamed posImages] |-processedNegTrainingImages |-[cropped and renamed negImages] You must make the positiveTrainingImages / negativeTrainingImages directories yourself. How to Use Collecting data We recommend having at least 100 postives images of your object (for example, a Right Way Sign), and at least 100 negative images. A good rule is to always have at least as many negative images as positive images. Positive images : Consists of your object of interest as the center of attention. Crop and adjust the images to focus on your object, where the image essentially acts as a Region of Interest (ROI) in which the HOG algorithm will build a feature vector from. Be careful of the object's degree of rotation. HOG works when descriptors have the same \"ratio\". For example, a right way sign may have a 3:1 ratio of length to width, but when greatly rotated about the y-axis, the area the sign appears in what would be a square, 1:1. Negative images : Images that do not contain your object of interest. A rule of thumb is to not choose purely random pictures as negatives, but images that represent backgrounds/environments in which the model will or may be used in. Provided are two simple helper Python scripts for image preprocessing (commands to run are found near the top of the files): batchRename.py : Copies, then renames and resizes all images within a given directory, saving these new images in a separate directory. feedSaver.py : Using a camera feed, it saves a specified number of frames from the feed as images, within a specified directory. Use the letter e key to start saving frames from the feed, and ESC to quit the stream. By default saves the images as .png, but can be changed to other image formats (ex. .jpg ). Choosing ML parameters For both the Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM), there are several parameters to be chosen to optimize your integrated model. For HOG (line 131, parameter in hog.compute() , in TrainHOG.cpp ): winstride : Window stride, a tuple of two values, determines the step size of the sliding window. Intuitively, a sliding window is a rectangular region of interest (of set length and width) that moves across an image (akin to a convolutional neural network). T The window grabs feature vectors, passes them to our SVM model for classification. padding : A tuple of two values. Determines how many pixels are added to the Region of Interest (ROI) BEFORE feature extraction. Can increase detector accuracy, but if the value is too large, will cause a performance hit. Full documentation is here . More info on SVM types here , useful for the SVM-Type parameter. For SVM - main parameters (starting line 387, parameter in SVM() , in TrainHOG.cpp ): gamma : Manipulates the data in 3-D space, making it easier to separate the data non-linearly, at the cost of data distortion as gamma grows. kernel Type : Determines the Kernel function used for separating classes. The key methods are linear vs. nonlinear seperation, depending on the datasets. C : Determines the degree of leninence for misclassification of classes in the model. The higher the C value, the more the model will try to not misclassify the HOG feature vectors. SVM Type : Determines whether the SVM focuses on classification error minimization vs. regression error minimization. Full documentation here For model detection (line 113, parameter in HOGDescriptor.detectMultiscale() , in detector.py ): winstride : See above for basic description. For real-time detection, this HEAVILY affects performance. Small strides such as (2,2) will be much slower than (4,4), as more windows to evaluate become computationally expensive. We recommend starting at (4,4) or (8,8) and adjusting for speed vs. accuracy. padding : See above. scale : Determines the number of layers within the image pyramid. An image pyramid represents the downsamples of the original image into smaller resultants, and detection is done at each level. This HEAVILY impacts the speed of the detector. The smaller the value of scale, the more layers are added to the image pyramid - increasing computation time. finalThreshold : This sets a lower bound for detection rectangle clusters. A cluster of rectangles must have ONE more rectangle than the number set by finalThreshold to be drawn. For example, if finalThreshold = 1, then clusters of at least 2 rectangles are drawn. Training data Once there is a positive and negative dataset formatted, we can compile our model via the single C++ file - TrainHOG.cpp. Compilation is more complicated Windows, requiring separate software from Microsoft called MSVC (an IDE). We recommend group members with Mac OS X and Native Linux machines to compile the C++ code. Check if the machines have g++ installed, though most machines have it by default. MAC OS and Linux If g++ is not installed: $ sudo apt-get update then $ sudo apt install g++ Windows For training on Windows (if no members use Mac OS or Linux): scp your hog-svm folder over to the racecar. ssh into the racecar (we compile on the car as it has g++ ). Follow the instructions below (starting with make ), and if a Gtk-WARNING : cannot open display**** error appears: Download the windows folder from the Google Drive link provided above. windows contains one file TrainHOGWin.cpp Delete the current TrainHOG.cpp , replace it with TrainHOGWin.cpp , then rename it to TrainHOG.cpp . Alternatively: Connect a monitor to the racecar, and train natively with the instructions below. All Operating Systems We have provided a makefile to simplify the commands for object file linking and executable construction, so to use the C++ code, in your terminal: 1. Type `make` in the directory containing the `makefile`. 2. Type `./TrainHOG` to see what flags are available and how to run your executable. The `./` is how C++ executable files are run. An example of a command would be $ ./TrainHOG -dw=160 -dh=80 -pd=./posImages -nd=./negImages -fn=\"TrafficDet160x80.xml\" -v True 3. The output file will be your model, in XML format. This will be loaded into `detector.py` for use with Python. Does the detector work? If testing on personal machines first, there is sanity check for whether the model is trained correctly. When the detector cannot find the object, the feed will be slow and have noticeable lag. The moment the object is detected, the feed becomes much smoother and FPS improves. Using the detector This final component requires a student implementation of a non-maximum suppression algorithm (NMS). If you were to run the detector with all bounding boxes drawn, the screen would be filled with boxes (to see: in detector.py , draw rectangles immediately after detector.detectMultiscale() . By literal definition, the algorithm seeks to \"suppress\" all false positive bounding boxes. We want to draw a final bounding box on areas with the most hits , where multiple bounding boxes overlap. Provided in detector.py are two implementations, almost usable but not quite. The first one is coordinate and area based ( point_non_max_suppression() ). Since HOGDescriptor.detectMultiscale() returns both bounding boxes and detection scores ( weights ) for those boxes, we can alter the function to take weights into account, at higher precedence than area. We recommend printing out the weights variable to discern score differences between boxes that detect the object, and those that do not. The second is OpenCV's Deep Neural Network implementation of NMS. The key area of focus is how the weights from our model relate to the score_threshold and nms_threshold . This function, cv2.dnn.NMSBoxes() , is generally used with convolutional neural network models (CNNs) such as YOLO . The threshold parameters of this function only draw boxes that are above in value. Full documentation here . Multi-object detection There are two options (Labels for objects can be returned via dictionary keyed to each detector): Run two HOGDescriptor() objects, each one with a different XML model. Computationally expensive, but with detection parameter adjustment and CUDA GPU acceleration (already implemented), resultant speed should suffice for max two objects. (Not Recommended within scope) Alter the C++ code. Train a HOGDescriptor() for each object. Push each HOGDescriptor() into a Mat (n-dimensional array), and push a corresponding label for each HOGDescriptor() into a separate Mat . The labels Mat is already created, and each label should be an int ID. Train a multiclass SVM() on these two Mat arrays. Depending on student progress, we MAY release a fully functioning model and the correct NMS algorithm.","title":"HOG SVM"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#warning","text":"Be sure to FIRST focus on the practice racetracks set up (if this is done during Week 4, before the Grand Final). Solely machine learning based object detection will NOT get a team through to the end. Ensure your team has a conceptual and practical plan of attack for the final course, and a backup implementation of the sign detection (we know most of you will try to detect signs with this model).","title":"Warning"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#histogram-of-oriented-gradients-hog","text":"If interested in the entire background of the algorithm, the paper is here . This algorithm extracts key features from an image or set of images. These images are generally pre-processed to normalize lighting and shadows. It then finds features based on the most extreme gradient changes between pixels, which are then grouped into cells for an aggregate weight and direction. Intuitively, these features are strongest around the edges of objects and pixel intensity changes. An example HOG descriptor : The original sign The computed descriptor","title":"Histogram of Oriented Gradients (HOG)"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#support-vector-machines-svm","text":"This is a supervised machine learning method, depending on labeled training data. Based on the provided data, an SVM tries to find an optimal hyperplane (based on user parameters) to separate data points into unique classes. Optimal, in this case, is defined by separating clusters of data while maximizing the distance from those points - known as maximizing the margin of the training data. The hyperplane is built in N-dimensional space, and our model will be staged in 2-D or 3-D space, depending on parameters. A classic linear SVM: A nonlinear separable SVM, when taken to a higher dimension:","title":"Support Vector Machines (SVM)"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#hog-svm-interaction","text":"The data points are feature vectors in our case. The feature vectors built by the HOG are fed into the SVM, where they are separated into classes of feature vectors. More to come.","title":"HOG-SVM Interaction"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#directory-setup","text":"Download this zip file here on Google Drive onto your computer. After a full setup, your folder directory should look like this: | - hog-svm |-TrainHOG.cpp |-detector.py |-batchRename.py |-feedSaver.py |-howToBuild.txt |-makefile |-positiveTrainingImages |-[your images] |-negativeTrainingImages |-[your images] |-processedPosTrainingImages |-[cropped and renamed posImages] |-processedNegTrainingImages |-[cropped and renamed negImages] You must make the positiveTrainingImages / negativeTrainingImages directories yourself.","title":"Directory Setup"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#how-to-use","text":"","title":"How to Use"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#collecting-data","text":"We recommend having at least 100 postives images of your object (for example, a Right Way Sign), and at least 100 negative images. A good rule is to always have at least as many negative images as positive images. Positive images : Consists of your object of interest as the center of attention. Crop and adjust the images to focus on your object, where the image essentially acts as a Region of Interest (ROI) in which the HOG algorithm will build a feature vector from. Be careful of the object's degree of rotation. HOG works when descriptors have the same \"ratio\". For example, a right way sign may have a 3:1 ratio of length to width, but when greatly rotated about the y-axis, the area the sign appears in what would be a square, 1:1. Negative images : Images that do not contain your object of interest. A rule of thumb is to not choose purely random pictures as negatives, but images that represent backgrounds/environments in which the model will or may be used in. Provided are two simple helper Python scripts for image preprocessing (commands to run are found near the top of the files): batchRename.py : Copies, then renames and resizes all images within a given directory, saving these new images in a separate directory. feedSaver.py : Using a camera feed, it saves a specified number of frames from the feed as images, within a specified directory. Use the letter e key to start saving frames from the feed, and ESC to quit the stream. By default saves the images as .png, but can be changed to other image formats (ex. .jpg ).","title":"Collecting data"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#choosing-ml-parameters","text":"For both the Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM), there are several parameters to be chosen to optimize your integrated model. For HOG (line 131, parameter in hog.compute() , in TrainHOG.cpp ): winstride : Window stride, a tuple of two values, determines the step size of the sliding window. Intuitively, a sliding window is a rectangular region of interest (of set length and width) that moves across an image (akin to a convolutional neural network). T The window grabs feature vectors, passes them to our SVM model for classification. padding : A tuple of two values. Determines how many pixels are added to the Region of Interest (ROI) BEFORE feature extraction. Can increase detector accuracy, but if the value is too large, will cause a performance hit. Full documentation is here . More info on SVM types here , useful for the SVM-Type parameter. For SVM - main parameters (starting line 387, parameter in SVM() , in TrainHOG.cpp ): gamma : Manipulates the data in 3-D space, making it easier to separate the data non-linearly, at the cost of data distortion as gamma grows. kernel Type : Determines the Kernel function used for separating classes. The key methods are linear vs. nonlinear seperation, depending on the datasets. C : Determines the degree of leninence for misclassification of classes in the model. The higher the C value, the more the model will try to not misclassify the HOG feature vectors. SVM Type : Determines whether the SVM focuses on classification error minimization vs. regression error minimization. Full documentation here For model detection (line 113, parameter in HOGDescriptor.detectMultiscale() , in detector.py ): winstride : See above for basic description. For real-time detection, this HEAVILY affects performance. Small strides such as (2,2) will be much slower than (4,4), as more windows to evaluate become computationally expensive. We recommend starting at (4,4) or (8,8) and adjusting for speed vs. accuracy. padding : See above. scale : Determines the number of layers within the image pyramid. An image pyramid represents the downsamples of the original image into smaller resultants, and detection is done at each level. This HEAVILY impacts the speed of the detector. The smaller the value of scale, the more layers are added to the image pyramid - increasing computation time. finalThreshold : This sets a lower bound for detection rectangle clusters. A cluster of rectangles must have ONE more rectangle than the number set by finalThreshold to be drawn. For example, if finalThreshold = 1, then clusters of at least 2 rectangles are drawn.","title":"Choosing ML parameters"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#training-data","text":"Once there is a positive and negative dataset formatted, we can compile our model via the single C++ file - TrainHOG.cpp. Compilation is more complicated Windows, requiring separate software from Microsoft called MSVC (an IDE). We recommend group members with Mac OS X and Native Linux machines to compile the C++ code. Check if the machines have g++ installed, though most machines have it by default.","title":"Training data"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#mac-os-and-linux","text":"If g++ is not installed: $ sudo apt-get update then $ sudo apt install g++","title":"MAC OS and Linux"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#windows","text":"For training on Windows (if no members use Mac OS or Linux): scp your hog-svm folder over to the racecar. ssh into the racecar (we compile on the car as it has g++ ). Follow the instructions below (starting with make ), and if a Gtk-WARNING : cannot open display**** error appears: Download the windows folder from the Google Drive link provided above. windows contains one file TrainHOGWin.cpp Delete the current TrainHOG.cpp , replace it with TrainHOGWin.cpp , then rename it to TrainHOG.cpp . Alternatively: Connect a monitor to the racecar, and train natively with the instructions below.","title":"Windows"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#all-operating-systems","text":"We have provided a makefile to simplify the commands for object file linking and executable construction, so to use the C++ code, in your terminal: 1. Type `make` in the directory containing the `makefile`. 2. Type `./TrainHOG` to see what flags are available and how to run your executable. The `./` is how C++ executable files are run. An example of a command would be $ ./TrainHOG -dw=160 -dh=80 -pd=./posImages -nd=./negImages -fn=\"TrafficDet160x80.xml\" -v True 3. The output file will be your model, in XML format. This will be loaded into `detector.py` for use with Python.","title":"All Operating Systems"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#does-the-detector-work","text":"If testing on personal machines first, there is sanity check for whether the model is trained correctly. When the detector cannot find the object, the feed will be slow and have noticeable lag. The moment the object is detected, the feed becomes much smoother and FPS improves.","title":"Does the detector work?"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#using-the-detector","text":"This final component requires a student implementation of a non-maximum suppression algorithm (NMS). If you were to run the detector with all bounding boxes drawn, the screen would be filled with boxes (to see: in detector.py , draw rectangles immediately after detector.detectMultiscale() . By literal definition, the algorithm seeks to \"suppress\" all false positive bounding boxes. We want to draw a final bounding box on areas with the most hits , where multiple bounding boxes overlap. Provided in detector.py are two implementations, almost usable but not quite. The first one is coordinate and area based ( point_non_max_suppression() ). Since HOGDescriptor.detectMultiscale() returns both bounding boxes and detection scores ( weights ) for those boxes, we can alter the function to take weights into account, at higher precedence than area. We recommend printing out the weights variable to discern score differences between boxes that detect the object, and those that do not. The second is OpenCV's Deep Neural Network implementation of NMS. The key area of focus is how the weights from our model relate to the score_threshold and nms_threshold . This function, cv2.dnn.NMSBoxes() , is generally used with convolutional neural network models (CNNs) such as YOLO . The threshold parameters of this function only draw boxes that are above in value. Full documentation here .","title":"Using the detector"},{"location":"curriculum/mod4/01_hog_svm/hog_svm_lab/#multi-object-detection","text":"There are two options (Labels for objects can be returned via dictionary keyed to each detector): Run two HOGDescriptor() objects, each one with a different XML model. Computationally expensive, but with detection parameter adjustment and CUDA GPU acceleration (already implemented), resultant speed should suffice for max two objects. (Not Recommended within scope) Alter the C++ code. Train a HOGDescriptor() for each object. Push each HOGDescriptor() into a Mat (n-dimensional array), and push a corresponding label for each HOGDescriptor() into a separate Mat . The labels Mat is already created, and each label should be an int ID. Train a multiclass SVM() on these two Mat arrays. Depending on student progress, we MAY release a fully functioning model and the correct NMS algorithm.","title":"Multi-object detection"},{"location":"intro/overview/","text":"Overview Welcome to the BWSI RACECAR course website! This page will outline the sections of this site and give you an idea of how the pages are organized. Curriculum This section contains several teaching modules for organizing the material. Module 0 teaches how to connect to and interact with the car. Module 1 teaches how to control the car. Module 2 teaches how to do image processing with OpenCV. Module 3 teaches how to detect your enviornment with AR tags and maps. Module 4 introduces machine learning. Cheatsheets This section contains several reference cards with useful commands and tips for various tools used in this course (i.e. a \"cheatsheet\"). While it is always useful to read the official documentation, these pages may help you figure out how to perform useful basic commands faster than the broader general purpose documentation. Setup This section contains information on how to build, install, and configure your vehicle. This serves primarily as a reference for others. As part of of the BWSI RACECAR class, you will be assigned a fully assembled, functional car. If during the class your car stops working, contact a TA. Resources Contains a set of useful documentation for various tools and sensors. A good place to get some additional information!","title":"Course Overview"},{"location":"intro/overview/#overview","text":"Welcome to the BWSI RACECAR course website! This page will outline the sections of this site and give you an idea of how the pages are organized.","title":"Overview"},{"location":"intro/overview/#curriculum","text":"This section contains several teaching modules for organizing the material. Module 0 teaches how to connect to and interact with the car. Module 1 teaches how to control the car. Module 2 teaches how to do image processing with OpenCV. Module 3 teaches how to detect your enviornment with AR tags and maps. Module 4 introduces machine learning.","title":"Curriculum"},{"location":"intro/overview/#cheatsheets","text":"This section contains several reference cards with useful commands and tips for various tools used in this course (i.e. a \"cheatsheet\"). While it is always useful to read the official documentation, these pages may help you figure out how to perform useful basic commands faster than the broader general purpose documentation.","title":"Cheatsheets"},{"location":"intro/overview/#setup","text":"This section contains information on how to build, install, and configure your vehicle. This serves primarily as a reference for others. As part of of the BWSI RACECAR class, you will be assigned a fully assembled, functional car. If during the class your car stops working, contact a TA.","title":"Setup"},{"location":"intro/overview/#resources","text":"Contains a set of useful documentation for various tools and sensors. A good place to get some additional information!","title":"Resources"},{"location":"references/lidar/","text":"LIDAR Sensor Hardware A LIDAR sensor is a distance sensor that spins around. Our particular sensor is the Hokuyo UST-10LX scanning laser rangefinder. Specs Wiring Sends data over ethernet Requires external power Data Detection range: 0.06 m to about 10 m Detection accuracy: +- 40 mm Scan angle range: 270\u00b0 Scan angle resolution: 1081 steps (\u2248 0.25\u00b0 increment between distance measurements) Scan speed: 25 ms (40 Hz) This sensor has an MSRP of 1.6 US kilodollars . Be aware of this as you conduct your testing. Usage in ROS We've already programmed the car to make a node that gets the LIDAR data and publishes it via messages to the /scan topic. Each message is of the type LaserScan . The actual scan data lies in the message's ranges attribute , which contains a list of all the distances (in meters) the LIDAR measured during one scan. Other LaserScan message attributes Some notable ones include angle_increment , angle_max , angle_min , range_max , range_min , scan_time , and intensities . For a full list, see: ros.org See the diagram to figure out the angles at which the distances are measured: How to visualize the data onscreen. In the car\u2019s terminal (ssh in if necessary), run teleop . In the computer\u2019s terminal (or car\u2019s if you have a monitor plugged in), run rviz . In rviz, select \"base_link\" from the \"frame\" dropdown menu. In rviz, press \"add\". In the popup, go to the \"By topic\" tab and select \"LaserScan\" from the \"\\scan\" topic. Hit the \"ok\" and enjoy! Important code snippets: #imports the dataype/class LaserScan from sensor_msgs.msg import LaserScan #a /scan topic subscriber laser_sub = rospy.Subscriber(\"/scan\", LaserScan, self.laser_callback, queue_size=1) #a callback function for the subscriber def laser_callback(scan_data): print(scan_data.ranges) If you're working inside another class, try using self.laser_sub = ... and def laser_callback(self, scan_data): ... instead.","title":"Lidar"},{"location":"references/lidar/#lidar-sensor","text":"","title":"LIDAR Sensor"},{"location":"references/lidar/#hardware","text":"A LIDAR sensor is a distance sensor that spins around. Our particular sensor is the Hokuyo UST-10LX scanning laser rangefinder. Specs Wiring Sends data over ethernet Requires external power Data Detection range: 0.06 m to about 10 m Detection accuracy: +- 40 mm Scan angle range: 270\u00b0 Scan angle resolution: 1081 steps (\u2248 0.25\u00b0 increment between distance measurements) Scan speed: 25 ms (40 Hz) This sensor has an MSRP of 1.6 US kilodollars . Be aware of this as you conduct your testing.","title":"Hardware"},{"location":"references/lidar/#usage-in-ros","text":"We've already programmed the car to make a node that gets the LIDAR data and publishes it via messages to the /scan topic. Each message is of the type LaserScan . The actual scan data lies in the message's ranges attribute , which contains a list of all the distances (in meters) the LIDAR measured during one scan. Other LaserScan message attributes Some notable ones include angle_increment , angle_max , angle_min , range_max , range_min , scan_time , and intensities . For a full list, see: ros.org See the diagram to figure out the angles at which the distances are measured: How to visualize the data onscreen. In the car\u2019s terminal (ssh in if necessary), run teleop . In the computer\u2019s terminal (or car\u2019s if you have a monitor plugged in), run rviz . In rviz, select \"base_link\" from the \"frame\" dropdown menu. In rviz, press \"add\". In the popup, go to the \"By topic\" tab and select \"LaserScan\" from the \"\\scan\" topic. Hit the \"ok\" and enjoy!","title":"Usage in ROS"},{"location":"references/lidar/#important-code-snippets","text":"#imports the dataype/class LaserScan from sensor_msgs.msg import LaserScan #a /scan topic subscriber laser_sub = rospy.Subscriber(\"/scan\", LaserScan, self.laser_callback, queue_size=1) #a callback function for the subscriber def laser_callback(scan_data): print(scan_data.ranges) If you're working inside another class, try using self.laser_sub = ... and def laser_callback(self, scan_data): ... instead.","title":"Important code snippets:"},{"location":"references/zed/","text":"Camera Sensor Hardware Each car has a ZED stereo camera . It can output a standard video stream, or it can output a depth estimate stream. Specs Wiring: Sends data and receives power via USB Data: Frames per second & resolution: 60fps @ 720p Field of view: 110\u00b0 Depth range: 0.5 - 20 m Can calculate odometry based on visual data This sensor has an MSRP of 0.5 US kilodollars . Be aware of this as you conduct your testing. Usage in ROS To use the ZED's data in ROS, first run `startZED` on the car's terminal . Beyond that, you usually will not have to interact with ZED data directly. Rather, we will often give you programs that process the ZED\u2019s data. For instance, in ZED.py , the \u201cimage_converter\u201d class takes in the raw data and converts it to CV2 format, which helps us greatly to process the image. For the curious though, we can get ZED data like we get other sensor data The ZED nodes publish different data to different topics. There are left/right camera topics and a depth topic. We often use `/zed/rgb/image_rect_color` . Each message is of the type `Image`. For a full list of `Image` message attributes, see ros.org . How to visualize the data onscreen. If you're ssh-ed into the car: on the car's terminal (i.e. ssh-in), run `startZED` on the computer's terminal, run `rqt_image_view` select the topic you would like to view If you have a monitor directly plugged in: Run `startZEDviz` in the terminal for a totally awesome visualization in RVIZ. Be sure to click on \"DepthCloud\" for the full effect.","title":"ZED"},{"location":"references/zed/#camera-sensor","text":"","title":"Camera Sensor"},{"location":"references/zed/#hardware","text":"Each car has a ZED stereo camera . It can output a standard video stream, or it can output a depth estimate stream. Specs Wiring: Sends data and receives power via USB Data: Frames per second & resolution: 60fps @ 720p Field of view: 110\u00b0 Depth range: 0.5 - 20 m Can calculate odometry based on visual data This sensor has an MSRP of 0.5 US kilodollars . Be aware of this as you conduct your testing.","title":"Hardware"},{"location":"references/zed/#usage-in-ros","text":"To use the ZED's data in ROS, first run `startZED` on the car's terminal . Beyond that, you usually will not have to interact with ZED data directly. Rather, we will often give you programs that process the ZED\u2019s data. For instance, in ZED.py , the \u201cimage_converter\u201d class takes in the raw data and converts it to CV2 format, which helps us greatly to process the image. For the curious though, we can get ZED data like we get other sensor data The ZED nodes publish different data to different topics. There are left/right camera topics and a depth topic. We often use `/zed/rgb/image_rect_color` . Each message is of the type `Image`. For a full list of `Image` message attributes, see ros.org . How to visualize the data onscreen. If you're ssh-ed into the car: on the car's terminal (i.e. ssh-in), run `startZED` on the computer's terminal, run `rqt_image_view` select the topic you would like to view If you have a monitor directly plugged in: Run `startZEDviz` in the terminal for a totally awesome visualization in RVIZ. Be sure to click on \"DepthCloud\" for the full effect.","title":"Usage in ROS"},{"location":"references/cartographer/cartographer_ref/","text":"Google Cartographer Installation Instructions New Installation Instructions Download this zip file here on Google Drive onto your computer and extract its contents. Then use scp to dump it onto the car into the car's Downloads: scp -r <path_to_my_computers_downloads_folder>/racecar_cartographer_installation racecar@192.168.1.<car_number>:~/Downloads/ Make sure your car's router is plugged into wifi. ssh into the car cd to the \"racecar_cartographer_installation\" folder Run the first shell script. (This replaces \"Install Google Cartographer\"): bash cartographer_install.sh Warning: this will take a long time ~ roughly 20-30 minutes. Run the second shell script. (This replaces \"Install MIT Racecar stuff\"): bash racecar_cartographer_install.sh Old Installation Instructions Install Google Cartographer Based on official Google Cartographer [instructions](https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html): # Update apt-get (good housekeeping) sudo apt-get update # Install ninja sudo apt-get install ninja-build python-wstool python-rosdep # Make a workspace for cartographer mkdir ~/cartographer_ws cd ~/cartographer_ws wstool init src # Fetch cartographer_ros wstool merge -t src https://raw.githubusercontent.com/googlecartographer/cartographer_ros/master/cartographer_ros.rosinstall wstool update -t src # Install proto3 src/cartographer/scripts/install_proto3.sh # Remove deb dependencies and reinitialize them sudo rm /etc/ros/rosdep/sources.list.d/20-default.list sudo rosdep init rosdep update # Must run from within \"cartographer_ws\" folder: rosdep install --from-paths src --ignore-src --rosdistro=${ROS_DISTRO} -y # Build and install. Must run from within \"cartographer_ws\" folder: catkin_make_isolated --install --use-ninja Then add these lines to the end of the car's \"~/.bashrc\" file if they're not already there: source ~/racecar_ws/.catkin_ws/devel/setup.bash source ~/cartographer_ws/install_isolated/setup.bash Install MIT Racecar stuff Clone this repo into your \"racecar_ws\" (not your \"cartographer_ws\"!) and catkin_make : cd ~/racecar_ws/.catkin_ws/src git clone https://github.com/mit-rss/cartographer_config.git cd ~/racecar_ws/.catkin_ws catkin_make source devel/setup.bash Then download this zip file here on Google Drive onto your computer and extract its contents. Then use scp to dump it onto the car into someplace logical (like the Downloads folder): scp -r <path_to_my_computers_downloads_folder>/racecar_cartographer_files racecar@192.168.1.<car_number>:~/Downloads/ Then on the racecar, cd into the resulting \"racecar_cartographer_files\" folder, and copy the files over into the following paths within \"cartographer_ws\": cp ./racecar_config_files/racecar_2d.lua ~/cartographer_ws/src/cartographer_ros/cartographer_ros/configuration_files/racecar_2d.lua cp ./racecar_config_files/racecar_2d_localization.lua ~/cartographer_ws/src/cartographer_ros/cartographer_ros/configuration_files/racecar_2d_localization.lua cp ./racecar_launch_files/racecar_2d.launch ~/cartographer_ws/src/cartographer_ros/cartographer_ros/launch/racecar_2d.launch cp ./racecar_launch_files/offline_racecar_2d.launch ~/cartographer_ws/src/cartographer_ros/cartographer_ros/launch/offline_racecar_2d.launch cp ./racecar_launch_files/demo_racecar_2d_localization.launch ~/cartographer_ws/src/cartographer_ros/cartographer_ros/launch/demo_racecar_2d_localization.launch cp -r racecar_description ~/cartographer_ws/src/ Finally catkin_make again to install these files: cd ~/cartographer_ws catkin_make_isolated --install --use-ninja Using Cartographer with ROS Note: These instructions assume you have installed Google Cartographer according to the above installation instructions. Making a Map from Live Data If you haven't already, make a folder to store maps. We recommend making it in the home directory mkdir ~/mapfiles . Run teleop . In another car's terminal, run source ~/cartographer_ws/devel_isolated/setup.bash , then roslaunch cartographer_ros racecar_2d.launch to start making the map. If you forget to source the setup file, you will get an error like: \"No such file or directory: /home/racecar/cartographer_ws/install_isolated/share/racecar_description/urdf/racecar.xacro...\" If you get another error along the lines of \"RLException: [racecar_2d.launch] is neither a launch file in package [cartographer_ros] nor is [cartographer_ros] a launch file name\", try cd -ing into \"~/cartographer_ws\" and running catkin_make_isolated --install --use-ninja . Run rviz , and add the \"\\map\" topic if it's not there already. Do this in Docker (make sure you ran it with the car number argument), or on your Ubuntu machine (with ROS installed), or on the car itself (if you have a monitor). If you are making a map with a rosbag, be warned that you will not see any map until you start playing the rosbag. To add a topic, click the \"Add\" button. Then go to the, \"By Topic\" tab, and add the topic you are interested in. Also, rviz can be finicky at times. If nothing appears even after running teleop or playing the rosbag, try changing the \"Fixed Frame\" to \"map\". Then check and uncheck the the checkboxes for the topics you are interested in. If that didn't work, try re-running Rviz. Check that you are running the programs you need to run. Drive the car around the area you wish to map out. Try to drive in closed loops when possible. When you are satisfied with your map, keep cartographer running. To save the map, run rosrun map_server map_saver -f ~/mapfiles/<your_map_name> Now you may kill cartographer. Making a Map from a Rosbag Rosbags are nice in that they allow you to isolate data collection from data processing. Recording the Rosbag. If you haven't already, make a folder to store rosbags. We recommend making it in the home directory mkdir ~/bagfiles . Place the car in a good starting position. In a car's terminal, run teleop . cd into your rosbag folder and run rosbag record -a -o <your_rosbag_name> to start recording data on all the topics. Drive the car around the area you wish to map out. Try to drive in closed loops when possible. When you are satisfied with your data collection (try to shoot for a minute or two of good data), kill the rosbag to stop recording. It may take a few seconds to stop, so let it die in peace. (optional) The bagfile naming system is kinda gross. Use an mv command to rename your bag file to something pretty. If you don't know what we mean by that, Google (and by that I mean DuckDuckGo or Ecosia ) \"renaming files in terminal\". Creating the Map To get a .pgm file and a .yaml file ( this is what our particle filter uses ): Follow the same instructions as for running off of live data, but: in step 2, instead of running teleop , run roscore in step 6, instead of driving the car around, run rosbag play ~/bagfiles/<your_rosbag_name>.bag Save the map when the rosbag stops playing. (You'll know it is done when it prints \"Done.\" in the terminal). Note: Remember to kill teleop! If you don't kill teleop, cartographer will see the rosbag data and current data at the same time! Plus, since the -a flag passed to rosbag record means record everything, playing the rosbag plays drive command data! Possible improvements for future TA's and students to explore: bag files take up a lot of memory; figure out which topics Cartographer uses and pass them as specific arguments to rosbag record . See rosbag documentation for details. Alternatively, to get a .pbstream file ( not recommended; this is usually for further use within Google Cartographer ): 1. Run roslaunch cartographer_ros offline_racecar_2d.launch bag_filenames:=${HOME}/bagfiles/<your_rosbag_name>.bag \u2002 Warning: this will pull up an rviz window. If you're ssh-ed in, then whoops. 2. Wait for the bag to finish playing, then watch the terminal and wait until it's done \"optimizing\".","title":"Cartographer"},{"location":"references/cartographer/cartographer_ref/#making-a-map-from-live-data","text":"If you haven't already, make a folder to store maps. We recommend making it in the home directory mkdir ~/mapfiles . Run teleop . In another car's terminal, run source ~/cartographer_ws/devel_isolated/setup.bash , then roslaunch cartographer_ros racecar_2d.launch to start making the map. If you forget to source the setup file, you will get an error like: \"No such file or directory: /home/racecar/cartographer_ws/install_isolated/share/racecar_description/urdf/racecar.xacro...\" If you get another error along the lines of \"RLException: [racecar_2d.launch] is neither a launch file in package [cartographer_ros] nor is [cartographer_ros] a launch file name\", try cd -ing into \"~/cartographer_ws\" and running catkin_make_isolated --install --use-ninja . Run rviz , and add the \"\\map\" topic if it's not there already. Do this in Docker (make sure you ran it with the car number argument), or on your Ubuntu machine (with ROS installed), or on the car itself (if you have a monitor). If you are making a map with a rosbag, be warned that you will not see any map until you start playing the rosbag. To add a topic, click the \"Add\" button. Then go to the, \"By Topic\" tab, and add the topic you are interested in. Also, rviz can be finicky at times. If nothing appears even after running teleop or playing the rosbag, try changing the \"Fixed Frame\" to \"map\". Then check and uncheck the the checkboxes for the topics you are interested in. If that didn't work, try re-running Rviz. Check that you are running the programs you need to run. Drive the car around the area you wish to map out. Try to drive in closed loops when possible. When you are satisfied with your map, keep cartographer running. To save the map, run rosrun map_server map_saver -f ~/mapfiles/<your_map_name> Now you may kill cartographer.","title":"Making a Map from Live Data"},{"location":"references/cartographer/cartographer_ref/#making-a-map-from-a-rosbag","text":"Rosbags are nice in that they allow you to isolate data collection from data processing.","title":"Making a Map from a Rosbag"},{"location":"references/cartographer/cartographer_ref/#recording-the-rosbag","text":"If you haven't already, make a folder to store rosbags. We recommend making it in the home directory mkdir ~/bagfiles . Place the car in a good starting position. In a car's terminal, run teleop . cd into your rosbag folder and run rosbag record -a -o <your_rosbag_name> to start recording data on all the topics. Drive the car around the area you wish to map out. Try to drive in closed loops when possible. When you are satisfied with your data collection (try to shoot for a minute or two of good data), kill the rosbag to stop recording. It may take a few seconds to stop, so let it die in peace. (optional) The bagfile naming system is kinda gross. Use an mv command to rename your bag file to something pretty. If you don't know what we mean by that, Google (and by that I mean DuckDuckGo or Ecosia ) \"renaming files in terminal\".","title":"Recording the Rosbag."},{"location":"references/cartographer/cartographer_ref/#creating-the-map","text":"To get a .pgm file and a .yaml file ( this is what our particle filter uses ): Follow the same instructions as for running off of live data, but: in step 2, instead of running teleop , run roscore in step 6, instead of driving the car around, run rosbag play ~/bagfiles/<your_rosbag_name>.bag Save the map when the rosbag stops playing. (You'll know it is done when it prints \"Done.\" in the terminal). Note: Remember to kill teleop! If you don't kill teleop, cartographer will see the rosbag data and current data at the same time! Plus, since the -a flag passed to rosbag record means record everything, playing the rosbag plays drive command data! Possible improvements for future TA's and students to explore: bag files take up a lot of memory; figure out which topics Cartographer uses and pass them as specific arguments to rosbag record . See rosbag documentation for details. Alternatively, to get a .pbstream file ( not recommended; this is usually for further use within Google Cartographer ): 1. Run roslaunch cartographer_ros offline_racecar_2d.launch bag_filenames:=${HOME}/bagfiles/<your_rosbag_name>.bag \u2002 Warning: this will pull up an rviz window. If you're ssh-ed in, then whoops. 2. Wait for the bag to finish playing, then watch the terminal and wait until it's done \"optimizing\".","title":"Creating the Map"},{"location":"references/collab/collab_ref/","text":"Working on Code Together! How can multiple people contribute to the same program? Option 1: Sketchy Website Advantages: tr\u00e8s easy to use Disadvantages: manual saving, kinda sketchy Host Instructions Go to collabedit.com . Click \"Create a New Document\". Enter a logical name. Select Python as your Programming Language. Share the url with your collaborators. Remember to copy your code to your host machine every time you want to save! Collaborator (everybody else) Instructions Get the url from your host. Enter a logical name. Remember to copy your code to your host machine every time you want to save! Option 2: Bloated Piece of Software Advantages: saves directly to local machine files (for the host, at least), looks fancy Disadvantages: gross and cluttered interface, confusing networking, becoming a minion of the Microsoft empire Instructions for Everybody Download, install, and run Visual Studio Code if you haven\u2019t already. Download/install/whatever the Live Share extension pack If you don\u2019t already have a Microsoft or GitHub account that you don\u2019t mind being spammed, create a garbage account with a garbage email. Save the username and password. If prompted, click \"allow\". VSCode should pop up. Click \"install\". Host Instructions Note: In this configuration, the host will be responsible for storing files and running them on the car. On the bottom tab thing, click \"Live Share\". If necessary, sign in with your Microsoft or Git account. When prompted, select the folder that holds your programs. This is like mounting, so you can edit the files on your host machine with another editor! Quickly! Before the popup goes away, copy the share link to your clipboard. If you do miss it, you can get it again from: Send the link to your collaborators. If something pops up, click \"allow\". It\u2019s probably your collaborators. Collaborator Instructions Click on the link your host sent. Hope something pops ups. If you want to save the code on your machine, you may need to do a manual copy-paste. Let us know if you figure out how to do something fancier! Option 3: DIY Whatever you do, you must be able to get all team members up and running such that they can see and edit code. Let us know if you have something really good.","title":"Collaboration"},{"location":"references/collab/collab_ref/#option-1-sketchy-website","text":"Advantages: tr\u00e8s easy to use Disadvantages: manual saving, kinda sketchy","title":"Option 1: Sketchy Website"},{"location":"references/collab/collab_ref/#host-instructions","text":"Go to collabedit.com . Click \"Create a New Document\". Enter a logical name. Select Python as your Programming Language. Share the url with your collaborators. Remember to copy your code to your host machine every time you want to save!","title":"Host Instructions"},{"location":"references/collab/collab_ref/#collaborator-everybody-else-instructions","text":"Get the url from your host. Enter a logical name. Remember to copy your code to your host machine every time you want to save!","title":"Collaborator (everybody else) Instructions"},{"location":"references/collab/collab_ref/#option-2-bloated-piece-of-software","text":"Advantages: saves directly to local machine files (for the host, at least), looks fancy Disadvantages: gross and cluttered interface, confusing networking, becoming a minion of the Microsoft empire","title":"Option 2: Bloated Piece of Software"},{"location":"references/collab/collab_ref/#instructions-for-everybody","text":"Download, install, and run Visual Studio Code if you haven\u2019t already. Download/install/whatever the Live Share extension pack If you don\u2019t already have a Microsoft or GitHub account that you don\u2019t mind being spammed, create a garbage account with a garbage email. Save the username and password. If prompted, click \"allow\". VSCode should pop up. Click \"install\".","title":"Instructions for Everybody"},{"location":"references/collab/collab_ref/#host-instructions_1","text":"Note: In this configuration, the host will be responsible for storing files and running them on the car. On the bottom tab thing, click \"Live Share\". If necessary, sign in with your Microsoft or Git account. When prompted, select the folder that holds your programs. This is like mounting, so you can edit the files on your host machine with another editor! Quickly! Before the popup goes away, copy the share link to your clipboard. If you do miss it, you can get it again from: Send the link to your collaborators. If something pops up, click \"allow\". It\u2019s probably your collaborators.","title":"Host Instructions"},{"location":"references/collab/collab_ref/#collaborator-instructions","text":"Click on the link your host sent. Hope something pops ups. If you want to save the code on your machine, you may need to do a manual copy-paste. Let us know if you figure out how to do something fancier!","title":"Collaborator Instructions"},{"location":"references/collab/collab_ref/#option-3-diy","text":"Whatever you do, you must be able to get all team members up and running such that they can see and edit code. Let us know if you have something really good.","title":"Option 3: DIY"},{"location":"references/docker/docker_ref/","text":"Docker Docker runs a mini virtual machine whose GUI (graphical user interface) can be viewed in a browser or VNC client. The point of Docker in this class is to get ROS onto non-Linux machines, so if you have something like Ubuntu or Arch-Linux, see these native ROS installation instructions instead . For those of you with non-Linux machines, the \"fishberg/racecar\" image already has an installation of ROS as well as a basic simulator for running code on virtual racecars. Unfortunately, a lot can go wrong with setting up the docker image. Be sure to check the troubleshooting sections for any errors that make pop up. Setup Step 1: Install Docker Based on your OS, follow the linked installation instructions below. Windows MacOS (You will have to make an account) Ubuntu (If you really want Docker anyways) Debian Fedora Step 2: Create a Mount Folder Create a folder to connect your Docker image to: # Windows (using Powershell) mkdir C:\\Users\\YOUR_USER_NAME\\mount mkdir C:\\Users\\YOUR_USER_NAME\\mount\\racecar_ws # MacOS mkdir -p ~/mount/racecar_ws # GNU/Linux mkdir -p ~/mount/racecar_ws What is a mounting? Being a \"light\" virtual machine, Docker machines do not by default write to any permanent memory on your computer, so without mounting, your Docker machine will always revert to the original \"fishberg/racecar\" image every time you restart Docker ( thus not saving any code you wrote on it! ). Mounting by contrast allows Docker to access and modify all the files in a mount folder on your host machine. We will use \"~/mount\" as the mount folder. Step 3: Install the Racecar Simulator Make a src folder in your racecar_ws : # Windows (using Powershell) mkdir C:\\Users\\YOUR_USER_NAME\\mount\\racecar_ws\\src # MacOS/GNU/Linux mkdir -p ~/mount/racecar_ws/src Clone the racecar code: cd ~/mount/racecar_ws/src git clone https://github.com/mit-racecar/racecar_simulator.git Make the catkin space: cd ~/mount/racecar_ws catkin_make source devel/setup.bash What is catkin_make ? It basically looks at the files in the \"src\" folder automatically generates and compiles stuff into the \"devel\" and \"build\" folders. For more details, Google it if you wish, but also, don't worry about it too much since it's not critical for programming the racecars. Using the Docker Image Running the Virtual Machine Start the generic docker image by running: # On Windows (run this either in the Docker Toolbox terminal or Powershell) docker run -ti --net=host -v /c/Users/<username>/mount/racecar_ws:/racecar_ws fishberg/racecar # On MacOS (make sure the Docker application is running!) sudo docker run -tip 6080:6080 -v ~/mount:/mnt fishberg/racecar # On GNU/Linux sudo docker run -ti --net=host -v ~/mount:/mnt fishberg/racecar The first time you run this command you will need to wait a little while for the image to download. Future runs should be instantaneous and won't require an internet connection. The image is currently ~2.25GB (ROS is big). If are using a specific car (for seeing the car's camera output or ssh'ing), you can add your car's number onto the end. For example, for Mac you would run sudo docker run -tip 6080:6080 -v ~/mount:/mnt fishberg/racecar <YOUR_CAR_NUMBER> The car number is the last part of the car's IP address (the IP should be of the form 192.168.1.<YOUR_CAR_NUMBER>). Running Troubleshooting Error something along the lines of \"[] is not a launch file nor a package...\" Remake the catkin space (see step 3 ); make sure you are in the racecar_ws. It may throw an error about cmake , but just try it again. If you then get a similar but different error... Try recloning the code (see step 3 ) and catkin_make again. Ubuntu: you might need to install some extra packages. Run apt-get install -y \\ ros-melodic-tf2-geometry-msgs \\ ros-melodic-ackermann-msgs \\ ros-melodic-joy \\ ros-melodic-map-server \\ build-essential Windows: try running one docker image for building spaces, and one for running launch files. In one Powershell, run docker run -ti --net=host -v /c/Users/YOUR_USERANME/mount:/mnt fishberg/racecar In \"racecar_ws\", catkin_make catkin_make #not a typo; do it twice source devel/setup.bash In a different Powershell, run docker run -ti --net=host -v /c/Users/YOUR_USERNAME/mount/racecar_ws:/racecar_ws/src fishberg/racecar In this docker image, you can run any roslaunch commands you need to. Mac error: \u201cError response from daemon: Bad response from Docker engine.\u201d Docker isn't running, start the application Windows error: \"...A connection attempt failed because the connected party did not properly respond after a period of time, ...\" Try rerunning the docker command in another Powershell window If an instructor deems it necessary, you may also try re-building the \"fishberg/racecar\" image from scratch. To build the image from scratch, run: git clone https://github.com/fishberg/racecar-docker.git cd racecar-docker sudo docker build -t racecar Then run with: sudo docker run -ti --net=host -v ~/mount:/mnt racecar Using the Image When you start up the virtual machine, you will be presented with a new bash shell in the folder racecar_ws . This shell will have ROS installed (e.g. try running roscore ). It also has the programs screen and tmux installed. These programs allow you to run many shells from within the same window. In addition to the terminal interface, you can interact with the image visually through either your browser or through VNC. This allows you to use programs like RViz. To use the image in the browser, navigate to http://192.168.99.100:6080/vnc.html (Windows) or http://localhost:6080/vnc.html (Mac). Hit the \"Connect\" button and you're in! The visual interface has two buttons that launch a terminal and RViz respectively. By default, clicking on the terminal button when a terminal is already open minimizes that window. To open multiple terminals, type CTRL and then click on the terminal icon. Be sure to check the troubleshooting section if it doesn't work. Running the Racecar Simulator To get started with the simulator, first run the following in any shell: roslaunch racecar_simulator simulate.launch Then open RViz. You should see a blue car on a black and white background (a map) and some colorful dots (simulated lidar). If you click the green 2D Pose Estimate arrow on the top you can change the position of the car. Alternatively use a joystick to drive the car as described below. Using a Joystick Unfortunately, we can currently only get a joystick to work on Linux and Windows machines due to Mac's different USB system. To use a joystick in the image (e.g. to use with the simulator), you need to forward inputs from that USB device into docker. Most joysticks map to \"/dev/input/js0\" by default, so you can add that device with the flag \"--device=/dev/input/js0\". For example, for Linux you can run sudo docker run -ti --net=host --device=/dev/input/js0 -v ~/mount:/mnt fishberg/racecar <YOUR_CAR_NUMBER> Usage Troubleshooting If the browser link doesn't work, use the provided link after you run the docker image in your terminal (it should be in the paragraph that appears after running the image). You can also try replacing the IP in the address with the results of typing hostname -I in the image's terminal. If a red banner appear at the top of the noVNC client saying \"Failed to connect to server\", close your broswer and try re-running the docker image. RViz: if your RViz isn't setup correctly (ex. you don't see the map or LIDAR scans), make sure the \"Displays\" option is checked under \"Panels\". Under the \"Global Settings\" drop-down, set the topic to \"map\"; under \"Map\", set the topic to \"/map\"; and under \"LaserScans\", set the size to 0.1. If there is no LaserScans, Map, or RobotModel drop-down, click on \"Add\" at the bottom of the left panel, and select all of them.","title":"Docker"},{"location":"references/docker/docker_ref/#setup","text":"","title":"Setup"},{"location":"references/docker/docker_ref/#step-1-install-docker","text":"Based on your OS, follow the linked installation instructions below. Windows MacOS (You will have to make an account) Ubuntu (If you really want Docker anyways) Debian Fedora","title":"Step 1: Install Docker"},{"location":"references/docker/docker_ref/#step-2-create-a-mount-folder","text":"Create a folder to connect your Docker image to: # Windows (using Powershell) mkdir C:\\Users\\YOUR_USER_NAME\\mount mkdir C:\\Users\\YOUR_USER_NAME\\mount\\racecar_ws # MacOS mkdir -p ~/mount/racecar_ws # GNU/Linux mkdir -p ~/mount/racecar_ws What is a mounting? Being a \"light\" virtual machine, Docker machines do not by default write to any permanent memory on your computer, so without mounting, your Docker machine will always revert to the original \"fishberg/racecar\" image every time you restart Docker ( thus not saving any code you wrote on it! ). Mounting by contrast allows Docker to access and modify all the files in a mount folder on your host machine. We will use \"~/mount\" as the mount folder.","title":"Step 2: Create a Mount Folder"},{"location":"references/docker/docker_ref/#step-3-install-the-racecar-simulator","text":"Make a src folder in your racecar_ws : # Windows (using Powershell) mkdir C:\\Users\\YOUR_USER_NAME\\mount\\racecar_ws\\src # MacOS/GNU/Linux mkdir -p ~/mount/racecar_ws/src Clone the racecar code: cd ~/mount/racecar_ws/src git clone https://github.com/mit-racecar/racecar_simulator.git Make the catkin space: cd ~/mount/racecar_ws catkin_make source devel/setup.bash What is catkin_make ? It basically looks at the files in the \"src\" folder automatically generates and compiles stuff into the \"devel\" and \"build\" folders. For more details, Google it if you wish, but also, don't worry about it too much since it's not critical for programming the racecars.","title":"Step 3: Install the Racecar Simulator"},{"location":"references/docker/docker_ref/#using-the-docker-image","text":"","title":"Using the Docker Image"},{"location":"references/docker/docker_ref/#running-the-virtual-machine","text":"Start the generic docker image by running: # On Windows (run this either in the Docker Toolbox terminal or Powershell) docker run -ti --net=host -v /c/Users/<username>/mount/racecar_ws:/racecar_ws fishberg/racecar # On MacOS (make sure the Docker application is running!) sudo docker run -tip 6080:6080 -v ~/mount:/mnt fishberg/racecar # On GNU/Linux sudo docker run -ti --net=host -v ~/mount:/mnt fishberg/racecar The first time you run this command you will need to wait a little while for the image to download. Future runs should be instantaneous and won't require an internet connection. The image is currently ~2.25GB (ROS is big). If are using a specific car (for seeing the car's camera output or ssh'ing), you can add your car's number onto the end. For example, for Mac you would run sudo docker run -tip 6080:6080 -v ~/mount:/mnt fishberg/racecar <YOUR_CAR_NUMBER> The car number is the last part of the car's IP address (the IP should be of the form 192.168.1.<YOUR_CAR_NUMBER>). Running Troubleshooting Error something along the lines of \"[] is not a launch file nor a package...\" Remake the catkin space (see step 3 ); make sure you are in the racecar_ws. It may throw an error about cmake , but just try it again. If you then get a similar but different error... Try recloning the code (see step 3 ) and catkin_make again. Ubuntu: you might need to install some extra packages. Run apt-get install -y \\ ros-melodic-tf2-geometry-msgs \\ ros-melodic-ackermann-msgs \\ ros-melodic-joy \\ ros-melodic-map-server \\ build-essential Windows: try running one docker image for building spaces, and one for running launch files. In one Powershell, run docker run -ti --net=host -v /c/Users/YOUR_USERANME/mount:/mnt fishberg/racecar In \"racecar_ws\", catkin_make catkin_make #not a typo; do it twice source devel/setup.bash In a different Powershell, run docker run -ti --net=host -v /c/Users/YOUR_USERNAME/mount/racecar_ws:/racecar_ws/src fishberg/racecar In this docker image, you can run any roslaunch commands you need to. Mac error: \u201cError response from daemon: Bad response from Docker engine.\u201d Docker isn't running, start the application Windows error: \"...A connection attempt failed because the connected party did not properly respond after a period of time, ...\" Try rerunning the docker command in another Powershell window If an instructor deems it necessary, you may also try re-building the \"fishberg/racecar\" image from scratch. To build the image from scratch, run: git clone https://github.com/fishberg/racecar-docker.git cd racecar-docker sudo docker build -t racecar Then run with: sudo docker run -ti --net=host -v ~/mount:/mnt racecar","title":"Running the Virtual Machine"},{"location":"references/docker/docker_ref/#using-the-image","text":"When you start up the virtual machine, you will be presented with a new bash shell in the folder racecar_ws . This shell will have ROS installed (e.g. try running roscore ). It also has the programs screen and tmux installed. These programs allow you to run many shells from within the same window. In addition to the terminal interface, you can interact with the image visually through either your browser or through VNC. This allows you to use programs like RViz. To use the image in the browser, navigate to http://192.168.99.100:6080/vnc.html (Windows) or http://localhost:6080/vnc.html (Mac). Hit the \"Connect\" button and you're in! The visual interface has two buttons that launch a terminal and RViz respectively. By default, clicking on the terminal button when a terminal is already open minimizes that window. To open multiple terminals, type CTRL and then click on the terminal icon. Be sure to check the troubleshooting section if it doesn't work. Running the Racecar Simulator To get started with the simulator, first run the following in any shell: roslaunch racecar_simulator simulate.launch Then open RViz. You should see a blue car on a black and white background (a map) and some colorful dots (simulated lidar). If you click the green 2D Pose Estimate arrow on the top you can change the position of the car. Alternatively use a joystick to drive the car as described below. Using a Joystick Unfortunately, we can currently only get a joystick to work on Linux and Windows machines due to Mac's different USB system. To use a joystick in the image (e.g. to use with the simulator), you need to forward inputs from that USB device into docker. Most joysticks map to \"/dev/input/js0\" by default, so you can add that device with the flag \"--device=/dev/input/js0\". For example, for Linux you can run sudo docker run -ti --net=host --device=/dev/input/js0 -v ~/mount:/mnt fishberg/racecar <YOUR_CAR_NUMBER> Usage Troubleshooting If the browser link doesn't work, use the provided link after you run the docker image in your terminal (it should be in the paragraph that appears after running the image). You can also try replacing the IP in the address with the results of typing hostname -I in the image's terminal. If a red banner appear at the top of the noVNC client saying \"Failed to connect to server\", close your broswer and try re-running the docker image. RViz: if your RViz isn't setup correctly (ex. you don't see the map or LIDAR scans), make sure the \"Displays\" option is checked under \"Panels\". Under the \"Global Settings\" drop-down, set the topic to \"map\"; under \"Map\", set the topic to \"/map\"; and under \"LaserScans\", set the size to 0.1. If there is no LaserScans, Map, or RobotModel drop-down, click on \"Add\" at the bottom of the left panel, and select all of them.","title":"Using the Image"},{"location":"references/localization/localization_ref/","text":"Particle Filter Localization Installation Instructions To install this particle filter ROS package: Make a localization folder in home directory and cd into it: mkdir ~/localization cd ~/localization Within \"~/localization,\" try: sudo pip install cython git clone http://github.com/kctess5/range_libc cd range_libc/pywrapper # once upon a time, some RSS teams were able to compile GPU ray casting methods # ./compile_with_cuda.sh # but now, with re-flashed cars, we are humbled back to the peasant days of regular compiling ./compile.sh Since compiling with cuda fails, we do regular compiling. Thankfully, this does not make the localization program prohibitively slow. Make a catkin workspace folder and cd in to it: mkdir ~/localization/localization_ws cd ~/localization/localization_ws Within \"~/localization/localization_ws,\" make a new source folder using wstool: wstool init src Install rosdep. rosdep install -r --from-paths src --ignore-src --rosdistro ${ROS_DISTRO} -y Then download this zip file here on Google Drive onto your computer and extract its contents. Then use scp to dump it onto the car into someplace logical (like the Downloads folder): scp -r <path_to_my_computers_downloads_folder>/particle_filter_files racecar@192.168.1.<car_number>:~/Downloads/ Then on the racecar, cd into the resulting \"particle_filter_files\" folder, and copy the files over into the following paths within \"localization\" (note that these files come from [this repo](https://github.com/mit-racecar/particle_filter)): cp -r ./rviz ~/localization cp ./.catkin_workspace ~/localization/localization_ws/ cp -r ./launch ~/localization/localization_ws/src cp -r ./maps ~/localization/localization_ws/src cp ./src/* ~/localization/localization_ws/src cp ./CMakeLists.txt ~/localization/localization_ws/src cp ./package.xml ~/localization/localization_ws/src Catkin make this thing! cd ~/localization/localization_ws catkin_make Using the Particle Filter Note: These instructions assume you have installed the Particle Filter according to the above installation instructions. Running Localization If you have followed the installation instructions as intended, the maps the particle filter uses will be in \"~/localization/localization_ws/src/maps\". Assuming you have a \".pgm\" file and a \".yaml\" file in your \"~/mapfile\" folder, then you can copy all these files with: cp ~/mapfiles/* ~/localization/localization_ws/src/maps To select which map to use for localization, you'll need to modify your \"map_server.launch\" file in \"~/localization/localization_ws/src/launch\". You may need to chmod it to edit it. Launch files essentially tell roslaunch how to run nodes in a package. The modification is simply replacing \"basement_fixed.map.yaml\" with the name of your \".yaml\" file. Debugging tip: We recommend keeping the default path \"$(find particle_filter)/maps/<yaml_file_name>.yaml\". If roslaunch doesn't seem to find the \".yaml\", double check the filename (no really, stop and do that). Then you can try putting in the full path: \"/home/racecar/localization/localization_ws/src/maps/<yaml_file_name>.yaml\". Do not try using \"~/\"! roslaunch does not know what \"~/\" means like the terminal shell does. Now we can get cooking! In the car's terminal, run teleop . Then in another tab/window, run: source ~/localization/localization_ws/devel/setup.bash roslaunch particle_filter localize.launch After the program prints \"\u2026Received first LiDAR message,\" it should start to print \"iters per sec: 20 possible: 21\" to confirm that it is getting scan data and making localization estimates. Debugging tips: We found that it is usually necessary for the vesc to be running completely (i.e. there\u2019s a good Traxxis battery) in order for this to work. If the roslaunch starts launching, but then returns an error message like \"cannot locate node of type particle_filter\" , it likely means that the \"particle_filter.py\" file in \"~/localization/localization_ws/src/\" needs executable permissions. You can give it these permissions by running chmod +x particle_filter.py . If the roslaunch starts launching, but then the python file returns an error message like \"ImportError: No module named range_libc\" , it likely means that the range_libc installation failed. Try it again according to our instructions [here](http://bwsi-racecar.com/maps/localization/particle_filter_installation/). Maybe the \"setup.py\" file in \"~/localization/range_libc/pywrapper/\" needs executable permissions. Also, just as with cartographer, you may open RViz. Interesting topics to try will be \"/map\", \"/scan\", and \"/pf/viz/particles\" topics. Wondering how to add topics in RViz? See step 7 of the \"Running off of live data\" section of our Cartographer page . Again, rviz can be finicky at times. If nothing appears even after running teleop or playing the rosbag, try changing the \"Fixed Frame\" to \"map\". Then check and uncheck the the checkboxes for the topics you are interested in. If that didn't work, try re-running Rviz. Check that you are running the programs you need to run. The car likely does not know where it is starting on the map. Give it an estimate of where it is using the \"2D Pose Estimate\" tool. Click on the map for position, drag for orientation. If you want to be extra fancy, you can have the car do this itself, by publishing a PoseWithCovarianceStamped message to the /initialpose topic. You can see what these messages look like by running rostopic echo /initialpose and doing a 2D pose estimate in RViz. A good idea would be to place the car in a fixed starting position and publish this known position to \"/initialpose\" when you press a button. Then you could press another button to change state and start running. (optional) Don\u2019t like your view locked to (0,0,0)? Make it follow the car by changing your frame to something on the car. First use the \"Focus Camera\" tool and click near the pose estimates (red arrows) to center the view on the car initially. Then change \"Target Frame\" to something on the car (like \"base_link\") to keep up with the car\u2019s changes in position. Using Pose Estimate Data in ROS This is where you get the pose estimate of where the car is on the map! Want to know where you are Subscribe to \"pf/viz/inferred_pose\"! To extract meaningful data from these messages, you can figure it out on your own. Use rostopic type to see what datatype the messages are. Once you have the name, you can find more info on ros.org . In your python code, remember to import the associated datatype: from geometry_msgs.msg import <datatype_of_inferred_pose> If you receive a ROS message in a python program and are unsure of what it is or what it contains, try printing it out. Quaternions Help (if you think angular info will help) You may have noticed the rotations for these ROS geometry messages are encoded in quaternions. Why? I really don\u2019t know, but it allows us to track the car\u2019s rotation from -2\u03c0 to 2\u03c0. If you care to amuse yourself for a few minutes, feel free to look up quaternions and derive the conversion back to an angle. Y'all are smart. Or you may just use the ROS\u2019s built-in transformations: from tf.transformations import euler_from_quaternion . . . def quatToAng3D(quat): euler = euler_from_quaternion((quat.x,quat.y,quat.z,quat.w)) return euler For reference, roll = euler[0] , pitch = euler[1] , yaw = euler[2] , and yaw is rotation about the z-axis. Google Cartographer Localization To run localization in Google Cartographer, you won't need an image and an \".yaml\" file, but rather this file structure called a \".pbstream\". Here's how you get this thing: (1). `cd` into the folder you want your \".pbstream\" stored. (2). Run `roslaunch cartographer_ros offline_racecar_2d.launch bag_filenames:=${HOME}/bagfiles/ .bag` \u2002 Warning: this will pull up an rviz window, so whoops if you're ssh-ed in. (3). Wait for the bag to finish playing, then watch the terminal and wait until it's done \"optimizing\". Now you wanna localize. Here's how you do something like that (though it also tries to make another map, which is concerning; maybe you need to modify one of the config files to include `max_submaps_to_keep = 3`, as the [Google Cartographer website](https://google-cartographer-ros.readthedocs.io/en/latest/going_further.html) suggests). (4). Run the localization by entering the following `roslaunch cartographer_ros demo_racecar_2d_localization.launch \\ load_state_filename:=${HOME}/ / .pbstream`. (5). We don't really know where to get pose data. And if you wanted to give the program pose estimated, good stinkin' luck, buddy. The best we can offer is intercepting stuff sent across the \"tf\" topic. While the localization is running, enter `rostopic echo tf`. The \"base_link\" frame may have relevant data. Change log (how did we concoct some of those launch and configuration files): (1). Copy the launch file demo_backpack_2d_localization.launch and rename it by entering `cp demo_backpack_2d_localization.launch demo_racecar_2d_localization.launch`. \u2002 Within this new file change robot_description to \"$(find xacro)/xacro '$(find racecar_description)/urdf/racecar.xacro'\")\" \u2002 Configuration_basename becomes racecar_2d_localization.lua \u2002 Don't remap from \"echoes\". Instead: \u2002 Remap from /odom to /vesc/odom \u2002 Remap from imu to /imu/datav (2). Delete the robag node. (3). First, enter `cp offline_backpack_2d.launch offline_racecar_2d.launch` Also, change the \"configuration_basename\" argument from backpack_2d.lua to racecar_2d.lua Delete the \"urdf_basename\" parameter entirely. Don't remap from \"echoes\". Instead: remap from /odom to /vesc/odom remap from imu to /imu/data","title":"Localization"},{"location":"references/localization/localization_ref/#using-the-particle-filter","text":"Note: These instructions assume you have installed the Particle Filter according to the above installation instructions.","title":"Using the Particle Filter"},{"location":"references/localization/localization_ref/#running-localization","text":"If you have followed the installation instructions as intended, the maps the particle filter uses will be in \"~/localization/localization_ws/src/maps\". Assuming you have a \".pgm\" file and a \".yaml\" file in your \"~/mapfile\" folder, then you can copy all these files with: cp ~/mapfiles/* ~/localization/localization_ws/src/maps To select which map to use for localization, you'll need to modify your \"map_server.launch\" file in \"~/localization/localization_ws/src/launch\". You may need to chmod it to edit it. Launch files essentially tell roslaunch how to run nodes in a package. The modification is simply replacing \"basement_fixed.map.yaml\" with the name of your \".yaml\" file. Debugging tip: We recommend keeping the default path \"$(find particle_filter)/maps/<yaml_file_name>.yaml\". If roslaunch doesn't seem to find the \".yaml\", double check the filename (no really, stop and do that). Then you can try putting in the full path: \"/home/racecar/localization/localization_ws/src/maps/<yaml_file_name>.yaml\". Do not try using \"~/\"! roslaunch does not know what \"~/\" means like the terminal shell does. Now we can get cooking! In the car's terminal, run teleop . Then in another tab/window, run: source ~/localization/localization_ws/devel/setup.bash roslaunch particle_filter localize.launch After the program prints \"\u2026Received first LiDAR message,\" it should start to print \"iters per sec: 20 possible: 21\" to confirm that it is getting scan data and making localization estimates. Debugging tips: We found that it is usually necessary for the vesc to be running completely (i.e. there\u2019s a good Traxxis battery) in order for this to work. If the roslaunch starts launching, but then returns an error message like \"cannot locate node of type particle_filter\" , it likely means that the \"particle_filter.py\" file in \"~/localization/localization_ws/src/\" needs executable permissions. You can give it these permissions by running chmod +x particle_filter.py . If the roslaunch starts launching, but then the python file returns an error message like \"ImportError: No module named range_libc\" , it likely means that the range_libc installation failed. Try it again according to our instructions [here](http://bwsi-racecar.com/maps/localization/particle_filter_installation/). Maybe the \"setup.py\" file in \"~/localization/range_libc/pywrapper/\" needs executable permissions. Also, just as with cartographer, you may open RViz. Interesting topics to try will be \"/map\", \"/scan\", and \"/pf/viz/particles\" topics. Wondering how to add topics in RViz? See step 7 of the \"Running off of live data\" section of our Cartographer page . Again, rviz can be finicky at times. If nothing appears even after running teleop or playing the rosbag, try changing the \"Fixed Frame\" to \"map\". Then check and uncheck the the checkboxes for the topics you are interested in. If that didn't work, try re-running Rviz. Check that you are running the programs you need to run. The car likely does not know where it is starting on the map. Give it an estimate of where it is using the \"2D Pose Estimate\" tool. Click on the map for position, drag for orientation. If you want to be extra fancy, you can have the car do this itself, by publishing a PoseWithCovarianceStamped message to the /initialpose topic. You can see what these messages look like by running rostopic echo /initialpose and doing a 2D pose estimate in RViz. A good idea would be to place the car in a fixed starting position and publish this known position to \"/initialpose\" when you press a button. Then you could press another button to change state and start running. (optional) Don\u2019t like your view locked to (0,0,0)? Make it follow the car by changing your frame to something on the car. First use the \"Focus Camera\" tool and click near the pose estimates (red arrows) to center the view on the car initially. Then change \"Target Frame\" to something on the car (like \"base_link\") to keep up with the car\u2019s changes in position.","title":"Running Localization"},{"location":"references/localization/localization_ref/#using-pose-estimate-data-in-ros","text":"This is where you get the pose estimate of where the car is on the map! Want to know where you are Subscribe to \"pf/viz/inferred_pose\"! To extract meaningful data from these messages, you can figure it out on your own. Use rostopic type to see what datatype the messages are. Once you have the name, you can find more info on ros.org . In your python code, remember to import the associated datatype: from geometry_msgs.msg import <datatype_of_inferred_pose> If you receive a ROS message in a python program and are unsure of what it is or what it contains, try printing it out. Quaternions Help (if you think angular info will help) You may have noticed the rotations for these ROS geometry messages are encoded in quaternions. Why? I really don\u2019t know, but it allows us to track the car\u2019s rotation from -2\u03c0 to 2\u03c0. If you care to amuse yourself for a few minutes, feel free to look up quaternions and derive the conversion back to an angle. Y'all are smart. Or you may just use the ROS\u2019s built-in transformations: from tf.transformations import euler_from_quaternion . . . def quatToAng3D(quat): euler = euler_from_quaternion((quat.x,quat.y,quat.z,quat.w)) return euler For reference, roll = euler[0] , pitch = euler[1] , yaw = euler[2] , and yaw is rotation about the z-axis.","title":"Using Pose Estimate Data in ROS"},{"location":"references/ros/ros_ref/","text":"Robot Operating System (ROS) & rospy Nodes Where do we give the robot its behavior? A node is a program that runs on the robot. ROS can run multiple nodes at the same time. For example, we give the racecars different nodes for their camera, LIDAR, and driving. The nodes can pass standardized messages between each other. For example, the driving nodes receive messages from the LIDAR and camera nodes. Punchline: this allows us to split up the robot's code into logical chunks (instead of having one clonky program )! This is the bee's knees for debugging, since one node can fail without causing others to crash. If we switch out a piece of hardware (e.g. if we change the type of LIDAR sensor) , we can just switch out the relevant chunks of software without scrounging through a huge program. Topics & Messages What carries data between nodes? Messages A message is a packet of data. To import the message's datatype : from my_source_file import my_msg_class To create a new message: my_msg = my_msg_class() This creates an instance of the my_msg_class class. Topics A topic is a communication channel that carries messages between nodes. For example, the LIDAR node sends its LaserScan messages to the /scan topic. Each topic can only carry messages of one datatype. Multiple nodes can publish to and/or subscribe to one topic. Here is our cheatsheet for topics and message types you will likely use. Publishers & Subscribers How exactly do nodes send/receive messages from topics? Publishers A publisher is a part of a node that can send messages to a topic. To initialize a publisher my_publisher = rospy.Publisher(my_topic, my_topics_datatype, queue_size=1) where queue_size will be given to you (when in doubt, make it 1). * To send a message self.my_publisher.publish(my_msg) Subscribers A subscriber is a part of a node that can receive messages from a topic. To initialize a subscriber: my_subscriber = rospy.Subscriber(my_topic, my_topics_datatype, my_callback_func) where my_callback_func is a callback function. The callback function's job is to process the received messages. Whenever the subscriber receives a message, it automatically calls the callback function and passes it the message. A callback function: def my_callback_func (a_msg_from_my_topic): print(a_msg_from_my_topic) Summaries and Related Material A mock example graph showing how a set of nodes might connect A graph showing how an actual racecar\u2019s nodes connect Command line tricks: see details on our ROS Cheatsheet . Some notable commands include: To see the connection diagram of nodes and topics, try rqt_graph . To list all the currently running nodes, try rosnode list . To list all the topics, try rostopic list . The Details of Connecting and Running Nodes NOTE: You will not need to know this program your cars (no really; the TA\u2019s were not even aware of this when we first worked on the cars), but it is kinda cool. Connecting Nodes The topics connect the nodes\u2026 But who builds the topics? Hiding under the hood is roscore . First roscore goes through all the nodes and looks for publishers. If it finds a publisher, it records what node it\u2019s in and what topic it publishes to. Then roscore goes through all the nodes and looks for subscribers. When it finds a subscriber, it checks to see if the subscriber\u2019s topic is in its list of publisher\u2019s topics. If there are publishers that publish to that topic, roscore will form a direct connection between the publisher(s) and the subscriber. Taken with modification by Avalon Vinella from \"Programming Robots with ROS\" published by O'Reilly Media Since roscore forms direct connections between publishers and subscribers, it\u2019s more like a telephone operator (which just connects lines/topics) than a post office (which intercepts all messages and sorts them back out). When do we actually run roscore ? See the last section. Running Nodes Thanks to the magic of rospy, all it takes to create a node is to run a python file containing rospy.init_node(\"my_node\") Running Nodes in Packages Sometimes it is inconvenient to run roscore all your nodes one by one. For convenience then, you can run roscore and a whole bunch of nodes automatically with teleop or startZED ; these are simplifications we've made using the car's .bashrc file (For reference, a bash file contains a set of terminal commands. The .bashrc file in particular automatically runs whenever you open a new terminal. In our case, the robot's main .bashrc file runs another bash file called .racecars ) In .racecars, we have written: ... # Create aliases to avoid having to type out ros packages and launch files alias teleop=\"roslaunch racecar teleop.launch\" alias startZED=\"roslaunch zed_wrapper zed.launch\" ... This makes running teleop equivalent to running roslaunch racecar teleop.launch in the car's terminal. roslaunch is the actual command we are using. It can run nodes or other files in the package its given, and if roscore is not already running, it runs roscore . racecar and zed_wrapper are ROS packages, a collection of files that can include nodes, launch files, or any other relevant files. teleop.launch and zed.launch are the launch files which tell roslaunch how to use the files in their respective packages.","title":"ROS"},{"location":"references/ros/ros_ref/#robot-operating-system-ros-rospy","text":"","title":"Robot Operating System (ROS) &amp; rospy"},{"location":"references/ros/ros_ref/#nodes","text":"Where do we give the robot its behavior? A node is a program that runs on the robot. ROS can run multiple nodes at the same time. For example, we give the racecars different nodes for their camera, LIDAR, and driving. The nodes can pass standardized messages between each other. For example, the driving nodes receive messages from the LIDAR and camera nodes. Punchline: this allows us to split up the robot's code into logical chunks (instead of having one clonky program )! This is the bee's knees for debugging, since one node can fail without causing others to crash. If we switch out a piece of hardware (e.g. if we change the type of LIDAR sensor) , we can just switch out the relevant chunks of software without scrounging through a huge program.","title":"Nodes"},{"location":"references/ros/ros_ref/#topics-messages","text":"What carries data between nodes?","title":"Topics &amp; Messages"},{"location":"references/ros/ros_ref/#messages","text":"A message is a packet of data. To import the message's datatype : from my_source_file import my_msg_class To create a new message: my_msg = my_msg_class() This creates an instance of the my_msg_class class.","title":"Messages"},{"location":"references/ros/ros_ref/#topics","text":"A topic is a communication channel that carries messages between nodes. For example, the LIDAR node sends its LaserScan messages to the /scan topic. Each topic can only carry messages of one datatype. Multiple nodes can publish to and/or subscribe to one topic. Here is our cheatsheet for topics and message types you will likely use.","title":"Topics"},{"location":"references/ros/ros_ref/#publishers-subscribers","text":"How exactly do nodes send/receive messages from topics?","title":"Publishers &amp; Subscribers"},{"location":"references/ros/ros_ref/#publishers","text":"A publisher is a part of a node that can send messages to a topic. To initialize a publisher my_publisher = rospy.Publisher(my_topic, my_topics_datatype, queue_size=1) where queue_size will be given to you (when in doubt, make it 1). * To send a message self.my_publisher.publish(my_msg)","title":"Publishers"},{"location":"references/ros/ros_ref/#summaries-and-related-material","text":"A mock example graph showing how a set of nodes might connect A graph showing how an actual racecar\u2019s nodes connect Command line tricks: see details on our ROS Cheatsheet . Some notable commands include: To see the connection diagram of nodes and topics, try rqt_graph . To list all the currently running nodes, try rosnode list . To list all the topics, try rostopic list .","title":"Summaries and Related Material"},{"location":"references/ros/ros_ref/#the-details-of-connecting-and-running-nodes","text":"NOTE: You will not need to know this program your cars (no really; the TA\u2019s were not even aware of this when we first worked on the cars), but it is kinda cool.","title":"The Details of Connecting and Running Nodes"},{"location":"references/ros/ros_ref/#running-nodes","text":"Thanks to the magic of rospy, all it takes to create a node is to run a python file containing rospy.init_node(\"my_node\")","title":"Running Nodes"},{"location":"references/ros/ros_ref/#running-nodes-in-packages","text":"Sometimes it is inconvenient to run roscore all your nodes one by one. For convenience then, you can run roscore and a whole bunch of nodes automatically with teleop or startZED ; these are simplifications we've made using the car's .bashrc file (For reference, a bash file contains a set of terminal commands. The .bashrc file in particular automatically runs whenever you open a new terminal. In our case, the robot's main .bashrc file runs another bash file called .racecars ) In .racecars, we have written: ... # Create aliases to avoid having to type out ros packages and launch files alias teleop=\"roslaunch racecar teleop.launch\" alias startZED=\"roslaunch zed_wrapper zed.launch\" ... This makes running teleop equivalent to running roslaunch racecar teleop.launch in the car's terminal. roslaunch is the actual command we are using. It can run nodes or other files in the package its given, and if roscore is not already running, it runs roscore . racecar and zed_wrapper are ROS packages, a collection of files that can include nodes, launch files, or any other relevant files. teleop.launch and zed.launch are the launch files which tell roslaunch how to use the files in their respective packages.","title":"Running Nodes in Packages"},{"location":"references/rosbag/rosbag_ref/","text":"Rosbags rosbag Files Sometimes, it isn\u2019t always time-efficient to take the car for a drive every time you want to test your algorithm. This will help with that problem. Essentially, we are creating a recording of messages that ROS is processing, and when we play this recording, it\u2019s as if the car itself is sending the messages in live time. Note: Make sure to only record the messages you need. Otherwise, the rosbag file will be too large and will take up more memory than we have on the car, which could lead to a system crash or a completely bricked car. Be careful when you record these. A good safety to have is to use the \u201c-d\u201d flag, for duration. Check this out in the documentation. All of this info and more is located in the rosbag documentation. To record: rosbag record -O .bag scan To playback: rosbag play .bag To play at a multiplied speed, like 2x speed, use the following command rosbag play -r 2 .bag There should be a bag file on the car, inside the ~/bagFiles directory. Try playing it right now. Open up the lidar visualizer [TODO: get instructions on how to open lidar visualizer], to see the data from the car driving through a hallway. If you get a \u201ctried to contact rosmaster\u201d error, or lidar data isn\u2019t consistent: Make sure teleop isn\u2019t running while you run a rosbag. Otherwise, your bagfile and the live lidar will be sending lidar messages, which will cause problems. Sometimes, killing teleop also kills the rosmaster node, which bagfiles need to run. To start the rosmaster node, run roscore is A final note: Ensure you are naming your rosbags well, otherwise your folder will quickly get crowded with messy files. Also, there isn\u2019t any reason to keep a bag file for a long time, so if you have a lot, go ahead and delete some. To visualize: On the car's terminals: Open roscore , open teleop , open rviz , stop teleop , run your rosbag.","title":"Rosbags"},{"location":"references/rosbag/rosbag_ref/#rosbags","text":"rosbag Files Sometimes, it isn\u2019t always time-efficient to take the car for a drive every time you want to test your algorithm. This will help with that problem. Essentially, we are creating a recording of messages that ROS is processing, and when we play this recording, it\u2019s as if the car itself is sending the messages in live time. Note: Make sure to only record the messages you need. Otherwise, the rosbag file will be too large and will take up more memory than we have on the car, which could lead to a system crash or a completely bricked car. Be careful when you record these. A good safety to have is to use the \u201c-d\u201d flag, for duration. Check this out in the documentation. All of this info and more is located in the rosbag documentation. To record: rosbag record -O .bag scan To playback: rosbag play .bag To play at a multiplied speed, like 2x speed, use the following command rosbag play -r 2 .bag There should be a bag file on the car, inside the ~/bagFiles directory. Try playing it right now. Open up the lidar visualizer [TODO: get instructions on how to open lidar visualizer], to see the data from the car driving through a hallway. If you get a \u201ctried to contact rosmaster\u201d error, or lidar data isn\u2019t consistent: Make sure teleop isn\u2019t running while you run a rosbag. Otherwise, your bagfile and the live lidar will be sending lidar messages, which will cause problems. Sometimes, killing teleop also kills the rosmaster node, which bagfiles need to run. To start the rosmaster node, run roscore is A final note: Ensure you are naming your rosbags well, otherwise your folder will quickly get crowded with messy files. Also, there isn\u2019t any reason to keep a bag file for a long time, so if you have a lot, go ahead and delete some. To visualize: On the car's terminals: Open roscore , open teleop , open rviz , stop teleop , run your rosbag.","title":"Rosbags"},{"location":"references/speakers/sound/","text":"Sound Tutorial We'll be using your car's speakers as a debugging tool. When you add a new state or driving mode to your car, you might want to add a new sound so you can be sure that the correct code is running. Look in soundNode.py and use the subscriber's topic to send your car's state to this node from other nodes. In order to get sound files onto the racecar, first download the file onto your own machine. From your computer ( not ssh'ed in to the car), use scp to copy the file over: scp /local/path/fileName.wav racecar@racecar:/<idkwhatthepathis>/sounds/fileName.wav","title":"Speakers"},{"location":"references/speakers/sound/#sound-tutorial","text":"We'll be using your car's speakers as a debugging tool. When you add a new state or driving mode to your car, you might want to add a new sound so you can be sure that the correct code is running. Look in soundNode.py and use the subscriber's topic to send your car's state to this node from other nodes. In order to get sound files onto the racecar, first download the file onto your own machine. From your computer ( not ssh'ed in to the car), use scp to copy the file over: scp /local/path/fileName.wav racecar@racecar:/<idkwhatthepathis>/sounds/fileName.wav","title":"Sound Tutorial"},{"location":"resources/contributors/","text":"TBD To be discussed with Sertac.","title":"Contributors"},{"location":"resources/contributors/#tbd","text":"To be discussed with Sertac.","title":"TBD"},{"location":"resources/faq/","text":"General Troubleshooting For a lot of these issues, the answer boils down to power cycling (turning the not-working-thing on and off again). So many things can be fixed by power cycling that before you decide something is not working, power cycle at least once and try again. Note: This applies to terminals too! Often \"teleop\" will give a Hokuyo or Vesc error which can be fixed by terminating the process and running teleop again. Car Hardware Issues Car does not turn on/dies when pushed: The first place to look is at the cars's batteries. Check that both your batteries are charged and fully connected. Then, check if the problem continues with a different battery. If the problem still persists, it may be the car's power harness that is broken. Use a spare power harness to test. If that oes not work, plug a TX2 battery charger straight into the TX2 motherboard. If no red light appears on the motherboard, your TX2 has been fried and its time to get another one. \"Teleop\" Errors Hokuyo/Vesc/IMU exception: Stop the process and try teleop again. If the issue persists, power cycle the car and try again. From here on out, either there could be an issue with the USB hub or the specific component that is throwing an error. If it's an issue with the USB hub, you will get both Vesc and IMU errors. Otherwise, it's an issue with your specific component. Make sure every one of your components are plugged in and has full battery if applicable. Otherwise, try borrowing spare components to see if its a hardware problem specific to your parts. Teleop does not throw error but your car doesn't drive: Is your controller on X mode? Did you get a \"force input\" error in teleop? If your controller was on D mode at any time after the car booted up, you need to switch the controller onto X mode and restart the car. You should also check that your T battery has charge, that your controller dongle is attached to the car, and that the Vesc has power. SSH Issues Cannot ssh into your car: make sure you are on the right wi-fi and that the car is on the right wi-fi (by logging into the car with a monitor). If the problem persists, make sure that your car and router were configured correctly (check the router configuration file). Laggy connection/connection turns off: make sure you are using the same wifi as the car. For example, if you are using the 5G, make sure the car is using the 5G as well. The lagginess can also be due to the amount of cars present which we can't do much about :( Switching off the 5G if everyone is using the 5G can help reduce the laginness, however. Lidar/Hokuyo Errors Make sure that you can hear the lidar making a spinning sound whenever the car is powered on. Then, make sure your ethernet cable is connected. Finally, make sure you have the correct ethernet settings from within the car (ie. the Hokuyo Zed Errors \"Camera is already in use\": your best bet is to shut the car down since there is probably a hidden ZED running on somebody's computer. That should fix the issue. OpenCV Error -215: Your code can't find the image! Trace back your image variable and make sure that it is compatible with openCV (is there a cv2.imread there?). If you are working with template images/outside images, make sure your image is actually on your machine and that your pathing is correct. If you are interfacing with ZED, make sure you have Zed up and running and that your camera is plugged in.","title":"FAQ"},{"location":"resources/faq/#general-troubleshooting","text":"For a lot of these issues, the answer boils down to power cycling (turning the not-working-thing on and off again). So many things can be fixed by power cycling that before you decide something is not working, power cycle at least once and try again. Note: This applies to terminals too! Often \"teleop\" will give a Hokuyo or Vesc error which can be fixed by terminating the process and running teleop again.","title":"General Troubleshooting"},{"location":"resources/faq/#car-hardware-issues","text":"Car does not turn on/dies when pushed: The first place to look is at the cars's batteries. Check that both your batteries are charged and fully connected. Then, check if the problem continues with a different battery. If the problem still persists, it may be the car's power harness that is broken. Use a spare power harness to test. If that oes not work, plug a TX2 battery charger straight into the TX2 motherboard. If no red light appears on the motherboard, your TX2 has been fried and its time to get another one.","title":"Car Hardware Issues"},{"location":"resources/faq/#teleop-errors","text":"Hokuyo/Vesc/IMU exception: Stop the process and try teleop again. If the issue persists, power cycle the car and try again. From here on out, either there could be an issue with the USB hub or the specific component that is throwing an error. If it's an issue with the USB hub, you will get both Vesc and IMU errors. Otherwise, it's an issue with your specific component. Make sure every one of your components are plugged in and has full battery if applicable. Otherwise, try borrowing spare components to see if its a hardware problem specific to your parts. Teleop does not throw error but your car doesn't drive: Is your controller on X mode? Did you get a \"force input\" error in teleop? If your controller was on D mode at any time after the car booted up, you need to switch the controller onto X mode and restart the car. You should also check that your T battery has charge, that your controller dongle is attached to the car, and that the Vesc has power.","title":"\"Teleop\" Errors"},{"location":"resources/faq/#ssh-issues","text":"Cannot ssh into your car: make sure you are on the right wi-fi and that the car is on the right wi-fi (by logging into the car with a monitor). If the problem persists, make sure that your car and router were configured correctly (check the router configuration file). Laggy connection/connection turns off: make sure you are using the same wifi as the car. For example, if you are using the 5G, make sure the car is using the 5G as well. The lagginess can also be due to the amount of cars present which we can't do much about :( Switching off the 5G if everyone is using the 5G can help reduce the laginness, however.","title":"SSH Issues"},{"location":"resources/faq/#lidarhokuyo-errors","text":"Make sure that you can hear the lidar making a spinning sound whenever the car is powered on. Then, make sure your ethernet cable is connected. Finally, make sure you have the correct ethernet settings from within the car (ie. the Hokuyo","title":"Lidar/Hokuyo Errors"},{"location":"resources/faq/#zed-errors","text":"\"Camera is already in use\": your best bet is to shut the car down since there is probably a hidden ZED running on somebody's computer. That should fix the issue.","title":"Zed Errors"},{"location":"resources/faq/#opencv","text":"Error -215: Your code can't find the image! Trace back your image variable and make sure that it is compatible with openCV (is there a cv2.imread there?). If you are working with template images/outside images, make sure your image is actually on your machine and that your pathing is correct. If you are interfacing with ZED, make sure you have Zed up and running and that your camera is plugged in.","title":"OpenCV"},{"location":"setup/hardware/build/","text":"TBD","title":"Build"},{"location":"setup/hardware/build/#tbd","text":"","title":"TBD"},{"location":"setup/hardware/router/","text":"Router Configuration Some of the routers have the actual TPLink firmware, but some of them have some other jank firmware that's a little bit harder to navigate. All of the routers need to be on the 1 subnet (192.168.1.XX vs the default 192.168.0.XX). If it's not, the Hokuyo ethernet connection will interfere with the car's router connection, and you won't be able to ssh in. Otherwise, the only other thing you might want to change is a static IP for your car, so that the IP won't change everytime you restart. You can do this by specifiying your car's mac address (found by either going to Network Information on the car directly or running ifconfig in a car terminal) and setting it to a certain IP (we usually stick to 192.168.1.YOUR_CAR_NUMBER).","title":"Router"},{"location":"setup/hardware/router/#router-configuration","text":"Some of the routers have the actual TPLink firmware, but some of them have some other jank firmware that's a little bit harder to navigate. All of the routers need to be on the 1 subnet (192.168.1.XX vs the default 192.168.0.XX). If it's not, the Hokuyo ethernet connection will interfere with the car's router connection, and you won't be able to ssh in. Otherwise, the only other thing you might want to change is a static IP for your car, so that the IP won't change everytime you restart. You can do this by specifiying your car's mac address (found by either going to Network Information on the car directly or running ifconfig in a car terminal) and setting it to a certain IP (we usually stick to 192.168.1.YOUR_CAR_NUMBER).","title":"Router Configuration"},{"location":"setup/software/jetson/","text":"TBD","title":"Jetson Config"},{"location":"setup/software/native_ros_install/native_ros_install_ref/","text":"Setup (Native ROS - Ubuntu/Debian) Step 1: Install ROS Based on your flavor of GNU/Linux, follow the linked installation instructions below. Be sure to install the ros-VERSION-desktop-full version of ROS. Install ROS on Ubuntu 18.04 Install ROS on Ubuntu 16.04 Install ROS on Debian Stretch Install ROS on Debian Jessie There is an experimental ROS installation for Arch Linux. While we love Arch, we've found the ROS package unreliable. If you must, you can follow the experimental instructions here . Step 2: Install Additional ROS Packages After ROS installation completes, install these additional ROS packages: # install on Ubuntu 18.04 & Debian Stretch sudo apt install ros-melodic-velodyne ros-melodic-ackermann-msgs ros-melodic-joy ros-melodic-serial # install on Ubuntu 16.04 & Debian Jessie sudo apt install ros-kinetic-velodyne ros-kinetic-ackermann-msgs ros-kinetic-joy ros-kinetic-serial Step 3: Install the racecar simulator code First make a racecar_ws : mkdir -p ~/racecar_ws/src Clone the racecar code: cd ~/racecar_ws/src git clone https://github.com/mit-racecar/racecar_simulator.git Make the code: cd ~/racecar_ws catkin_make source devel/setup.bash","title":"Native ROS Install"},{"location":"setup/software/native_ros_install/native_ros_install_ref/#step-1-install-ros","text":"Based on your flavor of GNU/Linux, follow the linked installation instructions below. Be sure to install the ros-VERSION-desktop-full version of ROS. Install ROS on Ubuntu 18.04 Install ROS on Ubuntu 16.04 Install ROS on Debian Stretch Install ROS on Debian Jessie There is an experimental ROS installation for Arch Linux. While we love Arch, we've found the ROS package unreliable. If you must, you can follow the experimental instructions here .","title":"Step 1: Install ROS"},{"location":"setup/software/native_ros_install/native_ros_install_ref/#step-2-install-additional-ros-packages","text":"After ROS installation completes, install these additional ROS packages: # install on Ubuntu 18.04 & Debian Stretch sudo apt install ros-melodic-velodyne ros-melodic-ackermann-msgs ros-melodic-joy ros-melodic-serial # install on Ubuntu 16.04 & Debian Jessie sudo apt install ros-kinetic-velodyne ros-kinetic-ackermann-msgs ros-kinetic-joy ros-kinetic-serial","title":"Step 2: Install Additional ROS Packages"},{"location":"setup/software/native_ros_install/native_ros_install_ref/#step-3-install-the-racecar-simulator-code","text":"First make a racecar_ws : mkdir -p ~/racecar_ws/src Clone the racecar code: cd ~/racecar_ws/src git clone https://github.com/mit-racecar/racecar_simulator.git Make the code: cd ~/racecar_ws catkin_make source devel/setup.bash","title":"Step 3: Install the racecar simulator code"}]}